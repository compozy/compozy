{
  "$defs": {
    "PromptParams": {
      "additionalProperties": false,
      "description": "PromptParams defines the parameters that control LLM behavior during text generation.",
      "properties": {
        "candidate_count": {
          "description": "CandidateCount specifies the number of response candidates to generate.\nSimilar to N but used by Google AI models.\n- **Range**: 1 to provider-specific maximum\n- **Provider Support**: Google AI (Gemini)",
          "type": "integer"
        },
        "frequency_penalty": {
          "description": "FrequencyPenalty penalizes tokens based on their frequency in the text so far.\nPositive values reduce repetition, negative values encourage it.\n- **Range**: -2.0 to 2.0\n- **Recommended**: 0.0 (no penalty) to 0.5 (moderate penalty)\n- **Provider Support**: OpenAI, Groq",
          "type": "number"
        },
        "max_length": {
          "description": "MaxLength provides an alternative way to specify maximum response length.\nTypically used by providers that distinguish between length and token limits.\n- **Range**: MinLength to provider-specific maximum\n- **Provider Support**: Primarily local models and some API providers",
          "type": "integer"
        },
        "max_tokens": {
          "description": "MaxTokens limits the maximum number of tokens in the generated response.\nThis parameter is crucial for cost control and response time management.\n- **Range**: 1 to model-specific maximum (e.g., 8192 for GPT-4)\n- **Default**: Provider-specific default (typically 1000-2000)",
          "type": "integer"
        },
        "metadata": {
          "description": "Metadata contains backend-specific parameters not covered by standard fields.\nUse this for provider-specific features or experimental parameters.\n- **Example**: Custom headers, request tracking, A/B test variants\n- **Provider Support**: Varies by provider",
          "type": "object"
        },
        "min_length": {
          "description": "MinLength specifies the minimum number of tokens that must be generated.\nPrevents the model from generating responses that are too short.\n- **Range**: 1 to MaxTokens\n- **Provider Support**: Limited; primarily local models",
          "type": "integer"
        },
        "n": {
          "description": "N specifies how many completion choices to generate for each prompt.\nUseful for generating multiple alternatives and selecting the best one.\n- **Range**: 1 to provider-specific maximum (typically 1-10)\n- **Default**: 1\n- **Provider Support**: OpenAI",
          "type": "integer"
        },
        "presence_penalty": {
          "description": "PresencePenalty penalizes tokens that have already appeared in the text.\nPositive values encourage the model to talk about new topics.\n- **Range**: -2.0 to 2.0\n- **Recommended**: 0.0 (no penalty) to 0.5 (moderate penalty)\n- **Provider Support**: OpenAI, Groq",
          "type": "number"
        },
        "repetition_penalty": {
          "description": "RepetitionPenalty reduces the likelihood of repeating the same tokens.\nValues \u003e 1.0 penalize repetition, values \u003c 1.0 encourage it.\n- **Range**: 0.1 to 2.0\n- **Recommended**: 1.0 (no penalty) to 1.2 (moderate penalty)\n- **Provider Support**: Primarily local models (Ollama, etc.)",
          "type": "number"
        },
        "seed": {
          "description": "Seed provides a random seed for reproducible outputs.\nWhen set, the same input with the same parameters will generate identical responses.\n- **Use Cases**: Testing, debugging, demonstration, A/B testing\n\u003e **Note:**: Not all providers support seeding; OpenAI and some others do",
          "type": "integer"
        },
        "stop_words": {
          "description": "StopWords defines a list of strings that will halt text generation when encountered.\nUseful for creating structured outputs or preventing unwanted content patterns.\n\n- **Example**: `[\"END\", \"STOP\", \"\\n\\n---\"]` for section-based content\n\u003e **Note:**: Not all providers support stop words; check provider documentation",
          "items": {
            "type": "string"
          },
          "type": "array"
        },
        "temperature": {
          "description": "Temperature controls the randomness of the generated text.\nLower values produce more deterministic, focused responses.\nHigher values increase creativity and variation but may reduce coherence.\n- **Range**: 0.0 (deterministic) to 1.0 (maximum randomness)\n- **Recommended**: 0.1-0.3 for factual tasks, 0.7-0.9 for creative tasks",
          "type": "number"
        },
        "top_k": {
          "description": "TopK limits the number of highest probability tokens considered during sampling.\nLower values focus on the most likely tokens, higher values allow more variety.\n- **Range**: 1 to vocabulary size (typically 1-100)\n- **Provider Support**: Primarily Google models and some local models",
          "type": "integer"
        },
        "top_p": {
          "description": "TopP (nucleus sampling) considers only tokens with cumulative probability up to this value.\nDynamically adjusts the vocabulary size based on probability distribution.\n- **Range**: 0.0 to 1.0\n- **Recommended**: 0.9 for balanced outputs, 0.95 for more variety",
          "type": "number"
        }
      },
      "type": "object"
    },
    "ProviderRateLimitConfig": {
      "additionalProperties": false,
      "description": "ProviderRateLimitConfig describes concurrency limits for a single provider.",
      "properties": {
        "concurrency": {
          "type": "integer"
        },
        "queue_size": {
          "type": "integer"
        },
        "requests_per_minute": {
          "description": "RequestsPerMinute limits average throughput; zero disables the limiter.",
          "type": "integer"
        },
        "tokens_per_minute": {
          "description": "TokensPerMinute constrains the total tokens consumed per minute; zero disables shaping.",
          "type": "integer"
        }
      },
      "type": "object"
    }
  },
  "$id": "provider.json",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "additionalProperties": false,
  "description": "ProviderConfig represents the complete configuration for an LLM provider in Compozy workflows.",
  "properties": {
    "api_key": {
      "description": "APIKey contains the authentication key for the AI provider.\n\n- **Security**: Use template references to environment variables.\n- **Examples**: `\"{{ .env.OPENAI_API_KEY }}\"`, `\"{{ .secrets.ANTHROPIC_KEY }}\"`\n\u003e **Note:**: Required for most cloud providers, optional for local providers",
      "type": "string"
    },
    "api_url": {
      "description": "APIURL specifies a custom API endpoint for the provider.\n**Use Cases**:\n  - Local model hosting (Ollama, OpenAI-compatible servers)\n  - Enterprise API gateways\n  - Regional API endpoints\n  - Custom proxy servers\n\n**Examples**: `\"http://localhost:11434\"`, `\"https://api.openai.com/v1\"`",
      "type": "string"
    },
    "context_window": {
      "description": "ContextWindow optionally overrides the provider's default context window size.\nWhen \u003e 0, this value replaces the provider's default ContextWindowTokens capability.\nUseful for providers like OpenRouter that support multiple models with varying limits.\n- **Example**: 200000 for Claude 3.5 Sonnet via OpenRouter\n- **Default**: Uses provider's default when not specified or \u003c= 0",
      "type": "integer"
    },
    "default": {
      "description": "Default indicates that this model should be used as the fallback when no explicit\nmodel configuration is provided at the task or agent level.\n\n**Behavior**:\n  - Only one model per project can be marked as default\n  - When set to true, this model will be used for tasks/agents without explicit model config\n  - Validation ensures at most one default model per project\n\n**Example**:\n```yaml\nmodels:\n  - provider: openai\n    model: gpt-4\n    default: true  # This will be used by default\n```",
      "type": "boolean"
    },
    "max_tool_iterations": {
      "description": "MaxToolIterations optionally caps the maximum number of tool-call iterations\nduring a single LLM request when tools are available.\nWhen \u003e 0, overrides the global default for this model; 0 uses the global default.",
      "type": "integer"
    },
    "model": {
      "description": "Model defines the specific model identifier to use with the provider.\nModel names are provider-specific and determine capabilities and pricing.\n\n- **Examples**:\n  - OpenAI: `\"gpt-4-turbo\"`, `\"gpt-3.5-turbo\"`\n  - Anthropic: `\"claude-4-opus\"`, `\"claude-3-5-haiku-latest\"`\n  - Google: `\"gemini-pro\"`, `\"gemini-pro-vision\"`\n  - Ollama: `\"llama2:13b\"`, `\"mistral:7b\"`",
      "type": "string"
    },
    "organization": {
      "description": "Organization specifies the organization ID for providers that support it.\n- **Primary Use**: OpenAI organization management for billing and access control\n\n- **Example**: `\"org-123456789abcdef\"`\n\u003e **Note:**: Optional for most providers",
      "type": "string"
    },
    "params": {
      "$ref": "#/$defs/PromptParams",
      "description": "Params contains the generation parameters that control LLM behavior.\nThese parameters are applied to all requests using this provider configuration.\nCan be overridden at the task or action level for specific requirements."
    },
    "provider": {
      "description": "Provider specifies which AI service to use for LLM operations.\nMust match one of the supported ProviderName constants.\n\n- **Examples**: `\"openai\"`, `\"anthropic\"`, `\"google\"`, `\"ollama\"`",
      "type": "string"
    },
    "rate_limit": {
      "$ref": "#/$defs/ProviderRateLimitConfig",
      "description": "RateLimit overrides concurrency limits and queue size for this provider.\nWhen omitted the orchestrator applies the global defaults."
    }
  },
  "title": "Provider Configuration",
  "type": "object",
  "yamlCompatible": true
}
