// Code generated by optionsgen. DO NOT EDIT.

package model

import core "github.com/compozy/compozy/engine/core"

type Option func(*core.ProviderConfig)

// WithProvider sets the Provider field
//
// Provider specifies which AI service to use for LLM operations.
// Must match one of the supported ProviderName constants.
// - **Examples**: `"openai"`, `"anthropic"`, `"google"`, `"ollama"`
func WithProvider(provider core.ProviderName) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.Provider = provider
	}
}

// WithModel sets the Model field
//
// Model defines the specific model identifier to use with the provider.
// Model names are provider-specific and determine capabilities and pricing.
// - **Examples**:
// - OpenAI: `"gpt-4-turbo"`, `"gpt-3.5-turbo"`
// - Anthropic: `"claude-4-opus"`, `"claude-3-5-haiku-latest"`
// - Google: `"gemini-pro"`, `"gemini-pro-vision"`
// - Ollama: `"llama2:13b"`, `"mistral:7b"`
func WithModel(model string) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.Model = model
	}
}

// WithAPIKey sets the APIKey field
//
// APIKey contains the authentication key for the AI provider.
// - **Security**: Use template references to environment variables.
// - **Examples**: `"{{ .env.OPENAI_API_KEY }}"`, `"{{ .secrets.ANTHROPIC_KEY }}"`
// > **Note:**: Required for most cloud providers, optional for local providers
func WithAPIKey(aPIKey string) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.APIKey = aPIKey
	}
}

// WithAPIURL sets the APIURL field
//
// APIURL specifies a custom API endpoint for the provider.
// **Use Cases**:
// - Local model hosting (Ollama, OpenAI-compatible servers)
// - Enterprise API gateways
// - Regional API endpoints
// - Custom proxy servers
// **Examples**: `"http://localhost:11434"`, `"https://api.openai.com/v1"`
func WithAPIURL(apiurl string) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.APIURL = apiurl
	}
}

// WithParams sets the Params field
//
// Params contains the generation parameters that control LLM behavior.
// These parameters are applied to all requests using this provider configuration.
// Can be overridden at the task or action level for specific requirements.
func WithParams(params core.PromptParams) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.Params = params
	}
}

// WithOrganization sets the Organization field
//
// Organization specifies the organization ID for providers that support it.
// - **Primary Use**: OpenAI organization management for billing and access control
// - **Example**: `"org-123456789abcdef"`
// > **Note:**: Optional for most providers
func WithOrganization(organization string) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.Organization = organization
	}
}

// WithDefault sets the Default field
//
// Default indicates that this model should be used as the fallback when no explicit
// model configuration is provided at the task or agent level.
// **Behavior**:
// - Only one model per project can be marked as default
// - When set to true, this model will be used for tasks/agents without explicit model config
// - Validation ensures at most one default model per project
// **Example**:
// ```yaml
// models:
// - provider: openai
// model: gpt-4
// default: true  # This will be used by default
// ```
func WithDefault(defaultValue bool) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.Default = defaultValue
	}
}

// WithMaxToolIterations sets the MaxToolIterations field
//
// MaxToolIterations optionally caps the maximum number of tool-call iterations
// during a single LLM request when tools are available.
// When > 0, overrides the global default for this model; 0 uses the global default.
func WithMaxToolIterations(maxToolIterations int) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.MaxToolIterations = maxToolIterations
	}
}

// WithRateLimit sets the RateLimit field
//
// RateLimit overrides concurrency limits and queue size for this provider.
// When omitted the orchestrator applies the global defaults.
func WithRateLimit(rateLimit *core.ProviderRateLimitConfig) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.RateLimit = rateLimit
	}
}

// WithContextWindow sets the ContextWindow field
//
// ContextWindow optionally overrides the provider's default context window size.
// When > 0, this value replaces the provider's default ContextWindowTokens capability.
// Useful for providers like OpenRouter that support multiple models with varying limits.
// - **Example**: 200000 for Claude 3.5 Sonnet via OpenRouter
// - **Default**: Uses provider's default when not specified or <= 0
func WithContextWindow(contextWindow int) Option {
	return func(cfg *core.ProviderConfig) {
		cfg.ContextWindow = contextWindow
	}
}
