---
description: "Guide for creating detailed task lists from PRDs and tech specs, including complexity analysis and phased generation workflow"
globs:
alwaysApply: false
---
# Rule: Generating a Task List from a PRD

<goal>
To guide an AI assistant in creating a detailed, step-by-step task list in Markdown format based on an existing Product Requirements Document (PRD `.md`) and its corresponding Technical Specification document (tech spec `.md`). The task list should guide a developer through implementation following both functional requirements and architectural decisions.
</goal>

<output_specification>
- **Format:** Markdown (`.md`)
- **Locations:**
  - **Feature folder:** `/tasks/prd-[feature-slug]/`
  - **Tasks summary file:** `/tasks/prd-[feature-slug]/_tasks.md`
  - **Individual task files:** `/tasks/prd-[feature-slug]/<num>_task.md`
  - **Complexity report:** `/tasks/prd-[feature-slug]/task-complexity-report.json`
</output_specification>

## Process

<process_steps>
1.  **Receive PRD Reference:** The user points the AI to a specific PRD file
2.  **Verify Tech Spec:** Confirm that a corresponding tech spec document exists (`tasks/prd-[feature-slug]/_techspec.md`). If not, remind the user to create it using [prd-tech-spec.mdc](mdc:.cursor/rules/prd-tech-spec.mdc) before proceeding with task generation.
3.  **Analyze PRD & Tech Spec:** The AI reads and analyzes both the functional requirements from the PRD and the implementation details from the tech spec document.
4.  **Phase 1: Generate Parent Tasks:** Based on the combined PRD and tech spec analysis, create the `_tasks.md` file in the feature folder with the relevant files section and high-level tasks. Use your judgement on how many high-level tasks to use. Present these tasks to the user.
5.  **Phase 2: Analyze Task Complexity:** Use Zen MCP tools to analyze the parent tasks' complexity. Generate a comprehensive complexity analysis report (typically saved as `task-complexity-report.json`) that includes complexity scores (1-10), recommended number of subtasks, reasoning for complexity assessment, and expansion prompts for each task. Tasks with complexity scores above the threshold (typically 6-7) should be broken down into more granular subtasks.
6.  **Wait for Confirmation:** Present the complexity analysis to the user and inform them: "I have generated the high-level tasks and analyzed their complexity. Ready to generate the sub-tasks based on this analysis? Respond with 'Go' to proceed."
7.  **Phase 3: Generate Sub-Tasks:** Once the user confirms, break down each parent task into smaller, actionable sub-tasks based on the complexity analysis results. Ensure sub-tasks logically follow from the parent task and cover the implementation details from both the PRD and tech spec.
8.  **Identify Relevant Files:** Based on the tasks, PRD, and tech spec, identify potential files that will need to be created or modified. Include files identified in the Tech Spec's Impact Analysis section as affected components. List these under the `Relevant Files` section, including corresponding test files if applicable. Reference the tech spec's file structure and architectural decisions.
9.  **Generate Final Output:** Create the individual task files and update the `_tasks.md` file.
10. **Create Individual Task Files:** For each parent task, create a separate file in `/tasks/prd-[feature-slug]/<num>_task.md` with:
    - Frontmatter with `status: pending` field
    - Task title and overview
    - Complete list of subtasks for that parent task
    - Implementation details extracted from the tech spec relevant to that task
    - Success criteria
11. **Update Tasks Summary:** Ensure the `_tasks.md` file contains:
    - Relevant files section (first)
    - Simple checkbox list of parent tasks (last)
12. **Save Complexity Report:** Save the complexity analysis as `task-complexity-report.json` in the feature folder.
</process_steps>

<complexity_analysis_features>
**The complexity report includes:**
- Metadata: generation timestamp, tasks analyzed count, threshold score, project name
- Per-task analysis: complexity scores with detailed reasoning
- Recommended subtask counts based on complexity assessment
- Specific expansion prompts for breaking down complex tasks
- Threshold-based recommendations for further subdivision
</complexity_analysis_features>

## Output Format

### Tasks Summary File Format
<tasks_summary_format>
The tasks summary file (`_tasks.md`) serves as the central index for the feature implementation, containing the relevant files and parent task checklist.
</tasks_summary_format>

### Individual Task File Format
<individual_task_format>
Each individual task file (`<num>_task.md`) must follow this structure:

```markdown
---
status: pending  # Options: pending, in-progress, completed, excluded
---

<task_context>
<domain>engine/infra/[subdomain]</domain>
<type>implementation|integration|testing|documentation</type>
<scope>core_feature|middleware|configuration|performance</scope>
<complexity>low|medium|high</complexity>
<dependencies>external_apis|database|temporal|http_server</dependencies>
</task_context>

# Task X.0: [Parent Task Title]

## Overview
[Brief description of what this task accomplishes]

## Subtasks

- [ ] X.1 [Subtask description]
- [ ] X.2 [Subtask description]
- [ ] X.3 [Subtask description]

## Implementation Details

[Extract relevant sections from the tech spec that apply to this task]

## Success Criteria
- [Clear definition of when this task is complete]
- [Measurable outcomes]
- [Quality requirements]
- [Components impacted by this task per Tech Spec Impact Analysis]

<critical>
**MANDATORY REQUIREMENTS:**

- **ALWAYS** verify against PRD and tech specs - NEVER make assumptions
- **NEVER** use workarounds, especially in tests - implement proper solutions
- **MUST** follow all established project standards:
  - Architecture patterns: `.cursor/rules/architecture.mdc`
  - Go coding standards: `.cursor/rules/go-coding-standards.mdc`
  - Testing requirements: `.cursor/rules/testing-standards.mdc`
  - API standards: `.cursor/rules/api-standards.mdc`
  - Security & quality: `.cursor/rules/quality-security.mdc`
- **MUST** run `make lint` and `make test` before completing parent tasks
- **MUST** follow `.cursor/rules/task-review.mdc` workflow for parent tasks

**Enforcement:** Violating these standards results in immediate task rejection.
</critical>
```
</individual_task_format>

### Tasks File Format
<tasks_file_format>
The tasks file (`_tasks.md`) must follow this structure:

```markdown
# [Feature] Implementation Task Summary

## Relevant Files

### Core Implementation Files
- `path/to/file.go` - Description
- `path/to/file_test.go` - Tests

### Integration Points
- `path/to/integration.go` - Description

### Documentation Files
- `docs/feature.md` - User documentation

### Notes

- Unit tests should be placed alongside the implementation files (e.g., `monitoring.go` and `monitoring_test.go` in the same directory)
- Use `go test ./...` to run all tests or `go test -v ./path/to/package` for specific package tests
- Always run `make fmt && make lint && make test` before committing changes
- Follow project testing standards with `t.Run("Should...")` pattern

## Tasks

- [ ] 1.0 Parent Task Title
- [ ] 2.0 Parent Task Title
- [ ] 3.0 Parent Task Title
```
</tasks_file_format>

## Interaction Model

<interaction_model>
The process explicitly requires a pause after generating parent tasks to get user confirmation ("Go") before proceeding to generate the detailed sub-tasks. This ensures the high-level plan aligns with user expectations before diving into details.
</interaction_model>

<target_audience>
Assume the primary reader of the task list is a **junior developer** who will implement the feature.
</target_audience>
