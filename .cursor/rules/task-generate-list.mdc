# Rule: Generating a Task List from a PRD

<goal>
To guide an AI assistant in creating a detailed, step-by-step task list in Markdown format based on an existing Product Requirements Document (PRD `.md`) and its corresponding Technical Specification document (tech spec `.md`). The task list should guide a developer through implementation following both functional requirements and architectural decisions.
</goal>

<source_of_truth_policy>
**SINGLE SOURCE OF TRUTH: Markdown Task Files**
- `_tasks.md` serves as the central task index and tracking file
- Individual `<num>_task.md` files contain detailed implementation guidance
- Task status updates are tracked by updating checkboxes in `_tasks.md`
- All task modifications should maintain consistency with the original PRD and tech spec analysis
- Task complexity and dependencies are documented in the complexity report
</source_of_truth_policy>

<output_specification>
- **Format:** Markdown (`.md`)
- **Locations:**
  - **Feature folder:** `/tasks/prd-[feature-slug]/`
  - **Tasks summary file:** `/tasks/prd-[feature-slug]/_tasks.md`
  - **Individual task files:** `/tasks/prd-[feature-slug]/<num>_task.md`
  - **Complexity report:** `/tasks/prd-[feature-slug]/task-complexity-report.md`
  - **Merged file:** `/tasks/prd-[feature-slug]/merged_prd_techspec.md`
</output_specification>

## Process

<process_steps>
1.  **Receive PRD Reference:** The user points the AI to a specific PRD file
2.  **Verify Tech Spec:** Confirm that a corresponding tech spec document exists (`tasks/prd-[feature-slug]/_techspec.md`). If not, remind the user to create it using [prd-tech-spec.mdc](mdc:.cursor/rules/prd-tech-spec.mdc) before proceeding with task generation.
3.  **Generate Merged File:** Run `mf ./tasks/prd-[feature-slug]` to generate a merged file containing the PRD + techspec (`merged_prd_techspec.md`)
4.  **Analyze PRD and Tech Spec:** Thoroughly analyze the merged file to understand all requirements, architectural decisions, and implementation details
5.  **Generate Task Structure:** Create a comprehensive task list based on the analysis, breaking down the implementation into logical parent tasks with subtasks
6.  **Generate Complexity Report:** Analyze each task's complexity considering factors like: technical difficulty, dependencies, integration points, testing requirements, and estimated effort
7.  **Generate Tasks Summary:** Create the `_tasks.md` file following the established format with all parent tasks listed
8.  **Parallel Agent Analysis:** ⚠️**MUST**: For each task under the prd-[slug], <critical>spin up parallel agents</critical> to provide a final analysis. Check based on the current architecture and files if we are duplicating stuff or forgetting something - a general analysis using our current architecture as review before starting the tasks
9.  **Wait for Confirmation:** Present the analysis results to the user and inform them: "I have completed the task generation and architectural analysis. Ready to proceed with implementation? Respond with 'Go' to start."
10. **Generate Individual Task Files:** Create separate files for each parent task in `/tasks/prd-[feature-slug]/<num>_task.md`
</process_steps>

<task_generation_guidelines>
**Task Creation Guidelines:**
1. **Break Down by Domain:** Group tasks by the domain they affect (e.g., agent, task, tool, workflow, infra)
2. **Logical Progression:** Order tasks so dependencies come before dependents
3. **Atomic Tasks:** Each parent task should be completable independently when its dependencies are met
4. **Clear Scope:** Each task should have a well-defined scope and deliverables
5. **Testing Included:** Include testing as subtasks within each parent task

**Complexity Analysis Factors:**
- **Technical Complexity:** Algorithm complexity, architectural changes, new patterns
- **Integration Points:** Number of systems/components that need to interact
- **Dependencies:** Both internal task dependencies and external system dependencies
- **Testing Requirements:** Unit, integration, and end-to-end testing needs
- **Risk Level:** Potential impact on existing functionality
</task_generation_guidelines>

<parallel_analysis_requirements>
**Critical Parallel Agent Analysis:**
For each task identified in the prd-[slug], you MUST spin up parallel agents to conduct:

1. **Architecture Duplication Check:** Identify if the task duplicates existing functionality
2. **Missing Component Analysis:** Check if the task overlooks existing architectural patterns
3. **Integration Point Validation:** Verify the task properly integrates with current systems
4. **Dependency Analysis:** Ensure all required dependencies are identified and accounted for
5. **Standards Compliance:** Verify the task follows established project patterns and standards

**Analysis Output:** Each parallel agent should provide:
- Duplication risks and recommendations
- Missing components or integration points
- Architectural alignment assessment
- Dependency validation results
- Standards compliance verification
</parallel_analysis_requirements>

## Output Format

### Tasks Summary File Format
<tasks_summary_format>
The tasks summary file (`_tasks.md`) serves as the central index for the feature implementation, containing the relevant files and parent task checklist. This file should be generated AFTER the LLM analysis and parallel agent review.
</tasks_summary_format>

### Individual Task File Format
<individual_task_format>
Each individual task file (`<num>_task.md`) must follow this structure:

```markdown
---
status: pending  # Options: pending, in-progress, completed, excluded
---

<task_context>
<domain>engine/infra/[subdomain]</domain>
<type>implementation|integration|testing|documentation</type>
<scope>core_feature|middleware|configuration|performance</scope>
<complexity>low|medium|high</complexity>
<dependencies>external_apis|database|temporal|http_server</dependencies>
</task_context>

# Task X.0: [Parent Task Title]

## Overview
[Brief description of what this task accomplishes]

<critical>
**MANDATORY REQUIREMENTS:**
- **ALWAYS** check dependent files APIs before write tests to avoid write wrong code
- **ALWAYS** verify against PRD and tech specs - NEVER make assumptions
- **NEVER** use workarounds, especially in tests - implement proper solutions
- **MUST** follow all established project standards:
    - Architecture patterns: `.cursor/rules/architecture.mdc`
    - Go coding standards: `.cursor/rules/go-coding-standards.mdc`
    - Testing requirements: `.cursor/rules/testing-standards.mdc`
    - API standards: `.cursor/rules/api-standards.mdc`
    - Security & quality: `.cursor/rules/quality-security.mdc`
- **MUST** run `make lint` and `make test` before completing ANY subtask
- **MUST** follow `.cursor/rules/task-review.mdc` workflow for parent tasks
**Enforcement:** Violating these standards results in immediate task rejection.
</critical>

## Subtasks

- [ ] X.1 [Subtask description]
- [ ] X.2 [Subtask description]
- [ ] X.3 [Subtask description]

## Implementation Details

[Extract relevant sections from the tech spec that apply to this task]

### Relevant Files

> List of relevant files that this task will touch

### Dependent Files

> List of dependencies that this task will need to handle

## Success Criteria
- [Clear definition of when this task is complete]
- [Measurable outcomes]
- [Quality requirements]
- [Components impacted by this task per Tech Spec Impact Analysis]
```
</individual_task_format>

### Tasks File Format
<tasks_file_format>
The tasks file (`_tasks.md`) must follow this structure and be generated AFTER the LLM analysis:

```markdown
# [Feature] Implementation Task Summary

## Relevant Files

### Core Implementation Files
- `path/to/file.go` - Description
- `path/to/file_test.go` - Tests

### Integration Points
- `path/to/integration.go` - Description

### Documentation Files
- `docs/feature.md` - User documentation

### Notes

- Unit tests should be placed alongside the implementation files (e.g., `monitoring.go` and `monitoring_test.go` in the same directory)
- Use `go test ./...` to run all tests or `go test -v ./path/to/package` for specific package tests
- Always run `make fmt && make lint && make test` before committing changes
- Follow project testing standards with `t.Run("Should...")` pattern

## Tasks

- [ ] 1.0 Parent Task Title
- [ ] 2.0 Parent Task Title
- [ ] 3.0 Parent Task Title
```
</tasks_file_format>

### Complexity Report Format
<complexity_report_format>
The complexity report (`task-complexity-report.md`) should analyze each task:

```markdown
# Task Complexity Analysis

## Summary
- Total Tasks: X
- High Complexity: X
- Medium Complexity: X
- Low Complexity: X

## Task Analysis

### Task 1.0: [Title]
- **Complexity:** High/Medium/Low
- **Factors:**
  - Technical complexity: [description]
  - Integration points: [count and description]
  - Dependencies: [list]
  - Testing requirements: [description]
- **Recommendation:** [whether to break down further or keep as is]

### Task 2.0: [Title]
[Similar structure for each task]
```
</complexity_report_format>

## Interaction Model

<interaction_model>
**Workflow with LLM Task Generation:**

1. **After Merged File Generation**: Verify the merged file contains both PRD and techspec content
2. **After LLM Analysis**: Review the generated task structure and complexity analysis
3. **After Parallel Agent Analysis**: Review architectural analysis results from all parallel agents
4. **Generate Markdown Files**: Create `_tasks.md`, individual task files, and complexity report
5. **Before Implementation**: Get user confirmation ("Go") after presenting complete analysis

**Critical Workflow Notes:**
- All tasks are generated by analyzing PRD and tech spec requirements
- Task modifications should maintain consistency with the original analysis
- Task status tracking happens through updating checkboxes in `_tasks.md`
- Individual task files provide detailed implementation guidance
- The complexity report guides task prioritization and resource allocation
</interaction_model>

<target_audience>
Assume the primary reader of the task list is a **junior developer** who will implement the feature, supported by the architectural analysis from parallel agents.
</target_audience>

<task_organization_policy>
**Task Organization Guidelines:**
- **Large Features**: For features with >10 parent tasks or high complexity, consider suggesting feature breakdown into phases
- **Task Grouping**: Group related tasks by domain or functional area for better organization
- **Naming Convention**: Use format `X.0` for parent tasks, `X.Y` for subtasks
- **Dependencies**: Clearly indicate task dependencies in both the individual task files and complexity report
- **Phased Approach**: For complex features, suggest implementation phases to manage risk and enable incremental delivery
</task_organization_policy>
