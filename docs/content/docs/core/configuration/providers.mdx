---
title: Providers
description: Configure LLM providers to enable AI agents to interact with various language models. Compozy supports 8 providers with distinct capabilities and configuration requirements.
---

<Callout type="info">
**Schema Reference**: For the most up-to-date and complete provider configuration schema, see the [Provider Schema Documentation](/docs/schema/provider). The schema contains all available properties, validation rules, and provider-specific requirements.
</Callout>

## Supported Providers

<List>
  <ListItem title="OpenAI">
    GPT-4, GPT-3.5, and other OpenAI models
  </ListItem>
  <ListItem title="Anthropic">
    Claude 3 family models for advanced reasoning
  </ListItem>
  <ListItem title="Google">
    Gemini Pro and other Google AI models
  </ListItem>
  <ListItem title="Groq">
    Fast inference platform with OpenAI-compatible API
  </ListItem>
  <ListItem title="Ollama">
    Local model hosting for self-hosted models
  </ListItem>
  <ListItem title="DeepSeek">
    DeepSeek AI models with OpenAI-compatible API
  </ListItem>
  <ListItem title="xAI">
    Grok models with OpenAI-compatible API
  </ListItem>
</List>

## Basic Configuration

Configure providers in the `models` section of your `compozy.yaml`:

```yaml
models:
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"

  - provider: anthropic
    model: claude-3-5-sonnet-20241022
    api_key: "{{ .env.ANTHROPIC_API_KEY }}"

  - provider: google
    model: gemini-pro
    api_key: "{{ .env.GOOGLE_API_KEY }}"
```

## Configuration Reference

### Core Properties

All providers support these core configuration properties. For the complete schema definition with all available fields and validation rules, see the [Provider Schema Documentation](/docs/schema/provider).

| Property | Type | Required | Description |
|----------|------|----------|-------------|
| `provider` | string | ✅ | Provider name (see supported providers above) |
| `model` | string | ✅ | Model identifier - any valid model string for the provider |
| `api_key` | string | ⚠️ | API key for authentication (required for cloud providers) |
| `api_url` | string | ❌ | Custom API endpoint URL |
| `organization` | string | ❌ | Organization ID (OpenAI only) |
| `params` | object | ❌ | Generation parameters (temperature, max_tokens, etc.) |

### Parameter Support

Parameters are specified in the `params` object. The available parameters vary by provider - consult the [Provider Schema Documentation](/docs/schema/provider) for provider-specific parameter schemas:

```yaml
models:
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    params:
      temperature: 0.7
      max_tokens: 4000
      top_p: 0.9
      seed: 42
```

<Callout type="warning">
**Parameter Compatibility**: Not all parameters are supported by all providers. Unsupported parameters are either ignored or cause configuration errors depending on the provider. Check the [Provider Schema Documentation](/docs/schema/provider) for detailed parameter compatibility.
</Callout>

## Provider-Specific Configuration

<Tabs items={["OpenAI", "Anthropic", "Google", "Groq", "Ollama", "DeepSeek", "xAI"]}>
  <Tab>
    **OpenAI Configuration**

    ```yaml
    models:
      - provider: openai
        model: gpt-4
        api_key: "{{ .env.OPENAI_API_KEY }}"
        organization: "org-123"                # Optional
        params:
          temperature: 0.7
          max_tokens: 4000
          top_p: 0.9
          seed: 42
    ```

    **Model Support:**
    Any valid OpenAI model string is supported. Popular models include:
    - `gpt-4` - Most capable model
    - `gpt-4-turbo` - Faster GPT-4 variant
    - `gpt-3.5-turbo` - Faster, cheaper option
    - `gpt-4o` - Multimodal capabilities
    - And any other model available in your OpenAI account

    **Error Conditions:**
    - ✅ All configuration options supported
    - ✅ Organization parameter supported
    - ✅ Custom API URL supported
  </Tab>

  <Tab>
    **Anthropic Configuration**

    ```yaml
    models:
      - provider: anthropic
        model: claude-3-5-sonnet-20241022
        api_key: "{{ .env.ANTHROPIC_API_KEY }}"
        params:
          temperature: 0.7
          max_tokens: 4000
    ```

    **Model Support:**
    Any valid Anthropic model string is supported. Popular models include:
    - `claude-3-5-sonnet-20241022` - Latest Claude 3.5 Sonnet
    - `claude-4-opus` - Most capable Claude 3
    - `claude-3-5-haiku-latest` - Fastest Claude 3
    - And any other model available in your Anthropic account

    **Error Conditions:**
    - ⚠️ **`organization` parameter causes error** - Do not use
    - ⚠️ **Custom `api_url` not supported** - Uses fixed endpoint
  </Tab>

  <Tab>
    **Google Configuration**

    ```yaml
    models:
      - provider: google
        model: gemini-pro
        api_key: "{{ .env.GOOGLE_API_KEY }}"
        params:
          temperature: 0.7
          max_tokens: 4000
          top_k: 40
          top_p: 0.95
    ```

    **Model Support:**
    Any valid Google AI model string is supported. Popular models include:
    - `gemini-pro` - Most capable Gemini model
    - `gemini-pro-vision` - Multimodal capabilities
    - `gemini-1.5-pro` - Latest Gemini version
    - And any other model available in your Google AI account

    **Error Conditions:**
    - ⚠️ **`organization` parameter causes error** - Do not use
    - ⚠️ **Custom `api_url` causes error** - Uses fixed endpoint
  </Tab>

  <Tab>
    **Groq Configuration**

    ```yaml
    models:
      - provider: groq
        model: llama3-8b-8192
        api_key: "{{ .env.GROQ_API_KEY }}"
        api_url: "https://api.groq.com/openai/v1"  # Optional
        organization: "org-123"                    # Optional
        params:
          temperature: 0.7
          max_tokens: 4000
    ```

    **Implementation:** OpenAI-compatible API
    **Default API URL:** `https://api.groq.com/openai/v1`
    **Error Conditions:**
    - ✅ All configuration options supported
    - ✅ Organization parameter supported
    - ✅ Custom API URL supported
  </Tab>

  <Tab>
    **Ollama Configuration**

    ```yaml
    models:
      - provider: ollama
        model: llama2:13b
        api_url: "http://localhost:11434"      # Required for custom host
        params:
          temperature: 0.8
          repetition_penalty: 1.1
    ```

    **Local Setup Required:** Ollama must be running locally or on specified host
    **Default API URL:** `http://localhost:11434`
    **Error Conditions:**
    - ❌ API key not required
    - ⚠️ **`organization` parameter causes error** - Do not use
    - ✅ Custom API URL supported
  </Tab>

  <Tab>
    **DeepSeek Configuration**

    ```yaml
    models:
      - provider: deepseek
        model: deepseek-chat
        api_key: "{{ .env.DEEPSEEK_API_KEY }}"
        api_url: "https://api.deepseek.com/v1"  # Optional
        organization: "org-123"                 # Optional
        params:
          temperature: 0.7
          max_tokens: 4000
    ```

    **Implementation:** OpenAI-compatible API
    **Default API URL:** `https://api.deepseek.com/v1`
    **Error Conditions:**
    - ✅ All configuration options supported
    - ✅ Organization parameter supported
    - ✅ Custom API URL supported
  </Tab>

  <Tab>
    **xAI Configuration**

    ```yaml
    models:
      - provider: xai
        model: grok-beta
        api_key: "{{ .env.XAI_API_KEY }}"
        api_url: "https://api.x.ai/v1"          # Optional
        organization: "org-123"                 # Optional
        params:
          temperature: 0.7
          max_tokens: 4000
    ```

    **Implementation:** OpenAI-compatible API
    **Default API URL:** `https://api.x.ai/v1`
    **Error Conditions:**
    - ✅ All configuration options supported
    - ✅ Organization parameter supported
    - ✅ Custom API URL supported
  </Tab>
</Tabs>

## Environment Variables

<Callout type="info">
Store API keys in environment variables, never in configuration files.
</Callout>

```bash
# .env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...
GROQ_API_KEY=gsk_...
DEEPSEEK_API_KEY=sk-...
XAI_API_KEY=xai-...
```

## Security Best Practices

<List>
  <ListItem title="API Key Management" icon="Key">
    Use environment variables for API keys and rotate them regularly
  </ListItem>
  <ListItem title="Network Security" icon="Shield">
    Use HTTPS endpoints and verify SSL certificates
  </ListItem>
  <ListItem title="Access Control" icon="Lock">
    Implement proper authentication and authorization
  </ListItem>
  <ListItem title="Input Validation" icon="CheckCircle">
    Validate and sanitize all inputs before sending to providers
  </ListItem>
  <ListItem title="Error Handling" icon="AlertTriangle">
    Handle provider errors gracefully and avoid exposing sensitive information
  </ListItem>
</List>

## Multiple Provider Configuration

You can configure multiple providers and models:

```yaml
models:
  # Primary provider
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    params:
      temperature: 0.7

  # Alternative provider
  - provider: anthropic
    model: claude-3-5-sonnet-20241022
    api_key: "{{ .env.ANTHROPIC_API_KEY }}"
    params:
      temperature: 0.5

  # Local provider for development
  - provider: ollama
    model: llama2:7b
    api_url: "http://localhost:11434"
    params:
      temperature: 0.8
```

<ReferenceCardList>
  <ReferenceCard
    title="Global Configuration"
    description="Configure system-wide settings and application defaults"
    href="/docs/core/configuration/global"
    icon="Settings"
  />
  <ReferenceCard
    title="Runtime Configuration"
    description="Set up execution environments and tool runtime settings"
    href="/docs/core/configuration/global"
    icon="Play"
  />
  <ReferenceCard
    title="Agent Configuration"
    description="Configure AI agents and their behavior"
    href="/docs/core/agents/overview"
    icon="Bot"
  />
  <ReferenceCard
    title="Workflow Configuration"
    description="Define workflow orchestration and task management"
    href="/docs/core/configuration/workflows"
    icon="Workflow"
  />
</ReferenceCardList>
