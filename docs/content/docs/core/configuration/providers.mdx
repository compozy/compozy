---
title: LLM Providers
description: Configure LLM providers and model settings for AI agents
---

# LLM Providers

Configure Large Language Model (LLM) providers and model settings for AI agents in Compozy.

## Overview

Compozy supports multiple LLM providers to give you flexibility in choosing the right model for your use case. You can configure multiple providers for:

- **Redundancy**: Fallback options if one provider is unavailable
- **Cost optimization**: Different models for different tasks
- **Performance tuning**: Fast models for simple tasks, powerful models for complex reasoning
- **Privacy**: Local models for sensitive data

## Supported Providers

### OpenAI

Configure OpenAI GPT models:

```yaml
models:
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    base_url: "https://api.openai.com/v1" # Optional
    org_id: "{{ .env.OPENAI_ORG_ID }}" # Optional
    temperature: 0.7
    max_tokens: 4000
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
```

**Available Models:**

- `gpt-4` - Most capable model
- `gpt-4-turbo` - Faster GPT-4 variant
- `gpt-3.5-turbo` - Fast and cost-effective
- `gpt-3.5-turbo-16k` - Extended context window

### Anthropic

Configure Anthropic Claude models:

```yaml
models:
  - provider: anthropic
    model: claude-3-opus-20240229
    api_key: "{{ .env.ANTHROPIC_API_KEY }}"
    base_url: "https://api.anthropic.com" # Optional
    temperature: 0.7
    max_tokens: 4000
    top_p: 1.0
    top_k: 40
```

**Available Models:**

- `claude-3-opus-20240229` - Most capable model
- `claude-3-sonnet-20240229` - Balanced performance
- `claude-3-haiku-20240307` - Fast and cost-effective
- `claude-instant-1.2` - Fastest model

### Google (Gemini)

Configure Google Gemini models:

```yaml
models:
  - provider: google
    model: gemini-pro
    api_key: "{{ .env.GOOGLE_API_KEY }}"
    base_url: "https://generativelanguage.googleapis.com/v1beta" # Optional
    temperature: 0.7
    max_tokens: 4000
    top_p: 1.0
    top_k: 40
```

**Available Models:**

- `gemini-pro` - Most capable text model
- `gemini-pro-vision` - Multimodal capabilities
- `gemini-ultra` - Ultra-large model (limited availability)

### Groq

Configure Groq for fast inference:

```yaml
models:
  - provider: groq
    model: llama-3.3-70b-versatile
    api_key: "{{ .env.GROQ_API_KEY }}"
    base_url: "https://api.groq.com/openai/v1" # Optional
    temperature: 0.7
    max_tokens: 4000
```

**Available Models:**

- `llama-3.3-70b-versatile` - Latest Llama model
- `llama-3.1-70b-versatile` - Balanced performance
- `llama-3.1-8b-instant` - Fast inference
- `mixtral-8x7b-32768` - Mixture of experts

### Ollama (Local)

Configure Ollama for local model inference:

```yaml
models:
  - provider: ollama
    model: llama2:13b
    api_url: "http://localhost:11434"
    temperature: 0.7
    num_predict: 4000
    top_p: 1.0
    top_k: 40
```

**Popular Local Models:**

- `llama2:7b` - Lightweight Llama 2
- `llama2:13b` - Balanced Llama 2
- `llama2:70b` - Large Llama 2
- `codellama:7b` - Code-specialized model
- `mistral:7b` - Efficient Mistral model

### Custom Providers

Configure custom OpenAI-compatible providers:

```yaml
models:
  - provider: custom
    model: custom-model
    api_key: "{{ .env.CUSTOM_API_KEY }}"
    base_url: "https://api.custom-provider.com/v1"
    temperature: 0.7
    max_tokens: 4000
    headers:
      X-Custom-Header: "custom-value"
```

## Configuration Fields

### Common Fields

| Field         | Type   | Description                                                     | Default          |
| ------------- | ------ | --------------------------------------------------------------- | ---------------- |
| `provider`    | string | Provider name (openai, anthropic, google, groq, ollama, custom) | -                |
| `model`       | string | Model identifier                                                | -                |
| `api_key`     | string | API key for authentication                                      | -                |
| `base_url`    | string | Custom API base URL                                             | Provider default |
| `temperature` | number | Sampling temperature (0.0-2.0)                                  | 0.7              |
| `max_tokens`  | number | Maximum tokens to generate                                      | 4000             |
| `top_p`       | number | Nucleus sampling parameter                                      | 1.0              |

### Provider-Specific Fields

#### OpenAI-specific

```yaml
models:
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    org_id: "{{ .env.OPENAI_ORG_ID }}"
    frequency_penalty: 0.0
    presence_penalty: 0.0
    stop: ["Human:", "Assistant:"]
    logit_bias: { "50256": -100 }
```

#### Anthropic-specific

```yaml
models:
  - provider: anthropic
    model: claude-3-opus-20240229
    api_key: "{{ .env.ANTHROPIC_API_KEY }}"
    top_k: 40
    stop_sequences: ["Human:", "Assistant:"]
```

#### Ollama-specific

```yaml
models:
  - provider: ollama
    model: llama2:13b
    api_url: "http://localhost:11434"
    num_predict: 4000
    num_ctx: 4096
    repeat_penalty: 1.1
    seed: 42
    tfs_z: 1.0
```

## Multi-Provider Configuration

Configure multiple providers for different use cases:

<Tabs items={['Multi-Provider', 'Cost Optimization', 'Performance Tiers']}>
  <Tab value="Multi-Provider">
```yaml
models:
  # Primary model for complex reasoning
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    temperature: 0.7
    max_tokens: 4000

# Fallback for high availability

- provider: anthropic
  model: claude-3-opus-20240229
  api_key: "{{ .env.ANTHROPIC_API_KEY }}"
  temperature: 0.7
  max_tokens: 4000

# Fast model for simple tasks

- provider: groq
  model: llama-3.1-8b-instant
  api_key: "{{ .env.GROQ_API_KEY }}"
  temperature: 0.3
  max_tokens: 2000

# Local model for sensitive data

- provider: ollama
  model: llama2:13b
  api_url: "http://localhost:11434"
  temperature: 0.7
  num_predict: 4000

````
  </Tab>
  <Tab value="Cost Optimization">
```yaml
models:
  # Cost-effective primary model
  - provider: openai
    model: gpt-3.5-turbo
    api_key: "{{ .env.OPENAI_API_KEY }}"
    temperature: 0.7
    max_tokens: 2000

  # Premium model for complex tasks
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    temperature: 0.7
    max_tokens: 4000

  # Ultra-fast for simple queries
  - provider: groq
    model: llama-3.1-8b-instant
    api_key: "{{ .env.GROQ_API_KEY }}"
    temperature: 0.3
    max_tokens: 1000
````

  </Tab>
  <Tab value="Performance Tiers">
```yaml
models:
  # Tier 1: Ultra-fast for simple tasks
  - provider: groq
    model: llama-3.1-8b-instant
    api_key: "{{ .env.GROQ_API_KEY }}"
    temperature: 0.3
    max_tokens: 1000

# Tier 2: Balanced for general use

- provider: openai
  model: gpt-3.5-turbo
  api_key: "{{ .env.OPENAI_API_KEY }}"
  temperature: 0.7
  max_tokens: 2000

# Tier 3: Premium for complex reasoning

- provider: openai
  model: gpt-4
  api_key: "{{ .env.OPENAI_API_KEY }}"
  temperature: 0.7
  max_tokens: 4000

# Tier 4: Ultra-premium for critical tasks

- provider: anthropic
  model: claude-3-opus-20240229
  api_key: "{{ .env.ANTHROPIC_API_KEY }}"
  temperature: 0.7
  max_tokens: 4000

````
  </Tab>
</Tabs>

## Agent-Specific Configuration

Configure different models for different agents:

```yaml
agents:
  - id: research-agent
    config:
      provider: openai
      model: gpt-4
      api_key: "{{ .env.OPENAI_API_KEY }}"
      temperature: 0.3  # Lower temperature for factual research
      max_tokens: 4000
    instructions: |
      You are a research assistant focused on accuracy and factual information.

  - id: creative-agent
    config:
      provider: anthropic
      model: claude-3-opus-20240229
      api_key: "{{ .env.ANTHROPIC_API_KEY }}"
      temperature: 0.9  # Higher temperature for creativity
      max_tokens: 4000
    instructions: |
      You are a creative writing assistant with a focus on originality.

  - id: code-agent
    config:
      provider: groq
      model: llama-3.1-70b-versatile
      api_key: "{{ .env.GROQ_API_KEY }}"
      temperature: 0.1  # Very low temperature for code
      max_tokens: 8000
    instructions: |
      You are a code assistant focused on writing clean, efficient code.
````

## Environment Variables

### Required Environment Variables

Set up API keys for your providers:

```bash
# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_ORG_ID=org-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...

# Google
GOOGLE_API_KEY=AIza...

# Groq
GROQ_API_KEY=gsk_...

# Custom provider
CUSTOM_API_KEY=...
```

### Optional Environment Variables

```bash
# Custom base URLs
OPENAI_BASE_URL=https://api.openai.com/v1
ANTHROPIC_BASE_URL=https://api.anthropic.com

# Model defaults
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=4000

# Ollama configuration
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
```

## Parameter Tuning

### Temperature

Controls randomness in the model's output:

```yaml
# Conservative (factual, consistent)
temperature: 0.1

# Balanced (default)
temperature: 0.7

# Creative (varied, imaginative)
temperature: 1.2
```

### Max Tokens

Controls the maximum length of generated responses:

```yaml
# Short responses
max_tokens: 500

# Standard responses
max_tokens: 2000

# Long-form content
max_tokens: 8000
```

### Top-P and Top-K

Fine-tune token selection:

```yaml
# Nucleus sampling (OpenAI, Anthropic)
top_p: 0.9

# Top-K sampling (Google, Ollama)
top_k: 40

# Combined (when supported)
top_p: 0.9
top_k: 40
```

## Best Practices

### Cost Optimization

1. **Use Appropriate Models**: Don't use GPT-4 for simple tasks
2. **Set Token Limits**: Prevent unexpectedly long responses
3. **Monitor Usage**: Track API costs and usage patterns
4. **Cache Results**: Cache expensive operations when possible

### Performance Optimization

1. **Choose Fast Models**: Use Groq for time-sensitive tasks
2. **Optimize Parameters**: Lower temperature for faster responses
3. **Local Models**: Use Ollama for high-throughput scenarios
4. **Parallel Processing**: Use multiple models concurrently

### Security

1. **Environment Variables**: Never hardcode API keys
2. **Key Rotation**: Regularly rotate API keys
3. **Access Control**: Limit API key permissions
4. **Local Models**: Use local models for sensitive data

### Reliability

1. **Multiple Providers**: Configure fallback providers
2. **Error Handling**: Implement robust error handling
3. **Rate Limiting**: Respect provider rate limits
4. **Monitoring**: Monitor provider availability and performance

## Common Patterns

### Fallback Chain

```yaml
models:
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    # Primary model

  - provider: anthropic
    model: claude-3-opus-20240229
    api_key: "{{ .env.ANTHROPIC_API_KEY }}"
    # Fallback if OpenAI fails

  - provider: ollama
    model: llama2:13b
    api_url: "http://localhost:11434"
    # Local fallback if all cloud providers fail
```

### Task-Specific Models

```yaml
models:
  # Fast model for classification
  - provider: groq
    model: llama-3.1-8b-instant
    api_key: "{{ .env.GROQ_API_KEY }}"

  # Balanced model for general tasks
  - provider: openai
    model: gpt-3.5-turbo
    api_key: "{{ .env.OPENAI_API_KEY }}"

  # Premium model for complex reasoning
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"

  # Code-specialized model
  - provider: groq
    model: llama-3.1-70b-versatile
    api_key: "{{ .env.GROQ_API_KEY }}"
```

### Development vs Production

<Tabs items={['Development', 'Production']}>
  <Tab value="Development">
```yaml
# Development configuration - cost-effective
models:
  - provider: ollama
    model: llama2:7b
    api_url: "http://localhost:11434"
    temperature: 0.7
    num_predict: 2000

- provider: openai
  model: gpt-3.5-turbo
  api_key: "{{ .env.OPENAI_API_KEY }}"
  temperature: 0.7
  max_tokens: 2000

````
  </Tab>
  <Tab value="Production">
```yaml
# Production configuration - high reliability
models:
  - provider: openai
    model: gpt-4
    api_key: "{{ .env.OPENAI_API_KEY }}"
    temperature: 0.7
    max_tokens: 4000

  - provider: anthropic
    model: claude-3-opus-20240229
    api_key: "{{ .env.ANTHROPIC_API_KEY }}"
    temperature: 0.7
    max_tokens: 4000

  - provider: groq
    model: llama-3.1-70b-versatile
    api_key: "{{ .env.GROQ_API_KEY }}"
    temperature: 0.7
    max_tokens: 4000
````

  </Tab>
</Tabs>

## Troubleshooting

### Common Issues

<Callout type="error">
  **Authentication Error**: Invalid API key ```bash # Solution: Verify API key
  is correct echo $OPENAI_API_KEY # Check environment variable ```
</Callout>

<Callout type="error">
  **Rate Limit Exceeded**: Too many requests ```bash # Solution: Implement rate
  limiting or use multiple keys ```
</Callout>

<Callout type="error">
  **Model Not Found**: Invalid model name ```bash # Solution: Check provider
  documentation for valid model names ```
</Callout>

### Debug Commands

```bash
# Test provider configuration
compozy config validate

# Test model availability
compozy model test gpt-4

# Monitor API usage
compozy model usage --provider openai

# Check model parameters
compozy model info gpt-4
```

## Next Steps

- Learn about [Agent Configuration](/docs/core/agents/overview)
- Set up [AutoLoad](/docs/core/configuration/autoload)
- Configure [Monitoring](/docs/core/configuration/monitoring-config)
- Explore [Tool Integration](/docs/core/tools/overview)
