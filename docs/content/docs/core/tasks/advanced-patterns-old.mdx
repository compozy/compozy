---
title: Advanced Patterns
description: Sophisticated task orchestration patterns for complex workflows and enterprise-scale applications
---

# Advanced Patterns

Master sophisticated task orchestration patterns that enable complex workflows, enterprise-scale applications, and advanced automation scenarios. This guide covers architectural patterns, optimization strategies, and real-world implementations using Compozy's 8 built-in execution types: basic, router, parallel, collection, composite, wait, signal, and memory.

## Related Documentation

### üîó Cross-References
- **[Core Concepts: Tasks](/docs/core/getting-started/core-concepts#2-tasks)** - Foundation task concepts
- **[Basic Tasks](/docs/core/tasks/basic-tasks)** - Building blocks for advanced patterns
- **[Composite Tasks](/docs/core/tasks/composite-tasks)** - Grouping and reusability
- **[Flow Control](/docs/core/tasks/flow-control)** - Advanced routing and control
- **[Business Workflow Examples](/docs/core/examples/business-workflows)** - Real-world implementations

### ‚öôÔ∏è Pattern-Related Topics
- **Advanced Patterns** ‚Üî **[Error Handling](/docs/core/tasks/basic-tasks#error-handling)** ‚Üî **[Signal Communication](/docs/core/signals/signal-overview)**
- **Orchestration** ‚Üî **[Parallel Processing](/docs/core/tasks/parallel-processing)** ‚Üî **[Collection Tasks](/docs/core/tasks/collection-tasks)**
- **Enterprise Patterns** ‚Üî **[Memory Management](/docs/core/tasks/memory-tasks)** ‚Üî **[Wait Tasks](/docs/core/tasks/wait-tasks)**

## Overview

Advanced patterns provide:

- **Complex workflow orchestration** with sophisticated control flow
- **Enterprise-scale patterns** for high-volume, distributed processing
- **Error resilience** with comprehensive fallback strategies
- **Performance optimization** for resource-intensive operations
- **Integration patterns** for complex system interactions
- **State management** for long-running, stateful processes

<Callout type="info">
These patterns build on the foundational execution types and demonstrate how to combine them into sophisticated workflows that can handle complex business requirements using Compozy's activity-based execution model.
</Callout>

<Callout type="warning">
**Complexity Notice**: The patterns in this guide are designed for advanced use cases and enterprise scenarios. For simpler workflows, consider starting with [Basic Tasks](/docs/core/tasks/basic-tasks) and [Collection Tasks](/docs/core/tasks/collection-tasks).
</Callout>

## Architectural Patterns

### 1. Multi-Stage Processing Pipeline

**Build sophisticated processing pipelines with error handling and state management:**

<Tabs items={['Pipeline Definition', 'Error Handling', 'State Management']}>
  <Tab value="Pipeline Definition">
```yaml
# Multi-stage data processing pipeline
- id: data_processing_pipeline
  type: composite
  description: "Enterprise data processing with validation and transformation"
  
  tasks:
    # Stage 1: Data Validation and Preparation
    - id: validation_stage
      type: parallel
      strategy: wait_all
      max_workers: 4
      tasks:
        - id: validate_format
          type: basic
          $use: tool(local::tools.#(id=="format_validator"))
          with:
            data: "{{ .workflow.input.raw_data }}"
            schema: "{{ .workflow.input.validation_schema }}"
        
        - id: validate_business_rules
          type: collection
          mode: parallel
          items: "{{ .workflow.input.business_rules }}"
          task:
            id: "validate_rule_{{ .index }}"
            type: basic
            $use: agent(local::agents.#(id=="rule_validator"))
            action: validate_rule
            with:
              rule: "{{ .item }}"
              data: "{{ .workflow.input.raw_data }}"
        
        - id: check_dependencies
          type: basic
          $use: tool(local::tools.#(id=="dependency_checker"))
          with:
            required_services: "{{ .workflow.input.dependencies }}"
    
    # Stage 2: Data Transformation
    - id: transformation_stage
      type: router
      condition: "{{ .tasks.validation_stage.output.all_valid }}"
      routes:
        true:
          type: parallel
          tasks:
            - id: transform_customer_data
              type: collection
              mode: parallel
              max_workers: 8
              items: "{{ .workflow.input.raw_data.customers }}"
              task:
                id: "transform_customer_{{ .index }}"
                type: basic
                $use: agent(local::agents.#(id=="data_transformer"))
                action: transform_customer
                with:
                  customer: "{{ .item }}"
                  transformation_rules: "{{ .workflow.input.transform_rules }}"
            
            - id: transform_order_data
              type: collection
              mode: parallel
              max_workers: 8
              items: "{{ .workflow.input.raw_data.orders }}"
              task:
                id: "transform_order_{{ .index }}"
                type: basic
                $use: tool(local::tools.#(id=="order_transformer"))
                with:
                  order: "{{ .item }}"
                  customer_mapping: "{{ .tasks.transform_customer_data.output }}"
        
        false:
          type: signal
          signal_name: "validation_failed"
          with:
            validation_errors: "{{ .tasks.validation_stage.output.errors }}"
            workflow_id: "{{ .workflow.id }}"
    
    # Stage 3: Data Aggregation and Quality Checks
    - id: aggregation_stage
      type: composite
      tasks:
        - id: aggregate_results
          type: aggregate
          inputs:
            - "{{ .tasks.transformation_stage.output.transform_customer_data }}"
            - "{{ .tasks.transformation_stage.output.transform_order_data }}"
          aggregation_strategy: "merge"
        
        - id: quality_assessment
          type: basic
          $use: agent(local::agents.#(id=="quality_assessor"))
          action: assess_quality
          with:
            aggregated_data: "{{ .tasks.aggregate_results.output }}"
            quality_thresholds: "{{ .workflow.input.quality_thresholds }}"
    
    # Stage 4: Final Processing and Storage
    - id: storage_stage
      type: router
      condition: "{{ .tasks.aggregation_stage.output.quality_assessment.quality_score }}"
      routes:
        "gt 0.8":
          type: parallel
          tasks:
            - id: store_primary
              type: basic
              $use: tool(local::tools.#(id=="primary_storage"))
              with:
                data: "{{ .tasks.aggregation_stage.output.aggregate_results }}"
                storage_config: "{{ .workflow.input.primary_storage }}"
            
            - id: store_backup
              type: basic
              $use: tool(local::tools.#(id=="backup_storage"))
              with:
                data: "{{ .tasks.aggregation_stage.output.aggregate_results }}"
                storage_config: "{{ .workflow.input.backup_storage }}"
        
        "default":
          type: signal
          signal_name: "quality_review_required"
          with:
            quality_score: "{{ .tasks.aggregation_stage.output.quality_assessment.quality_score }}"
            data_summary: "{{ .tasks.aggregation_stage.output.aggregate_results.summary }}"
```

This pipeline demonstrates:
- **Multi-stage processing** with clear separation of concerns
- **Conditional routing** based on validation and quality assessment
- **Parallel execution** for performance optimization
- **State management** with intermediate results
- **Error handling** with signal-based notifications

</Tab>
  <Tab value="Error Handling">
```yaml
# Comprehensive error handling pattern
- id: resilient_processing_workflow
  type: composite
  description: "Workflow with comprehensive error handling and recovery"
  
  # Global error handler
  on_error:
    next: global_error_handler
  
  tasks:
    # Primary processing path
    - id: primary_processing
      type: parallel
      strategy: best_effort
      max_workers: 6
      timeout: 10m
      
      # Individual task error handling
      tasks:
        - id: process_high_priority
          type: basic
          $use: tool(local::tools.#(id=="high_priority_processor"))
          with:
            data: "{{ .workflow.input.high_priority_data }}"
          on_error:
            next: high_priority_fallback
        
        - id: process_medium_priority
          type: basic
          $use: tool(local::tools.#(id=="medium_priority_processor"))
          with:
            data: "{{ .workflow.input.medium_priority_data }}"
          on_error:
            next: medium_priority_fallback
        
        - id: process_low_priority
          type: basic
          $use: tool(local::tools.#(id=="low_priority_processor"))
          with:
            data: "{{ .workflow.input.low_priority_data }}"
          on_error:
            next: low_priority_fallback
    
    # Fallback handlers for each priority level
    - id: high_priority_fallback
      type: router
      condition: "{{ .error.type }}"
      routes:
        "timeout":
          type: basic
          $use: tool(local::tools.#(id=="timeout_recovery"))
          with:
            original_data: "{{ .workflow.input.high_priority_data }}"
            timeout_strategy: "retry_with_smaller_batch"
        
        "service_unavailable":
          type: basic
          $use: tool(local::tools.#(id=="alternative_processor"))
          with:
            data: "{{ .workflow.input.high_priority_data }}"
            fallback_service: "secondary_high_priority"
        
        "default":
          type: signal
          signal_name: "manual_intervention_required"
          with:
            priority: "high"
            error_details: "{{ .error }}"
            data_context: "{{ .workflow.input.high_priority_data }}"
    
    - id: medium_priority_fallback
      type: wait
      condition: "{{ .system.resources.cpu_usage }}"
      max_wait: 5m
      processor:
        type: basic
        $use: tool(local::tools.#(id=="resource_aware_processor"))
        with:
          data: "{{ .workflow.input.medium_priority_data }}"
          processing_mode: "conservative"
    
    - id: low_priority_fallback
      type: memory
      operation: store
      key: "failed_low_priority_{{ .workflow.id }}"
      value: "{{ .workflow.input.low_priority_data }}"
      ttl: 24h
    
    # Global error handler
    - id: global_error_handler
      type: composite
      tasks:
        - id: log_error
          type: basic
          $use: tool(local::tools.#(id=="error_logger"))
          with:
            error_context: "{{ .error }}"
            workflow_state: "{{ .workflow }}"
            timestamp: "{{ now }}"
        
        - id: notify_operators
          type: basic
          $use: tool(local::tools.#(id=="notification_service"))
          with:
            alert_type: "workflow_failure"
            severity: "{{ .error.severity }}"
            message: "Workflow {{ .workflow.id }} failed: {{ .error.message }}"
        
        - id: trigger_recovery
          type: router
          condition: "{{ .error.recoverable }}"
          routes:
            true:
              type: signal
              signal_name: "initiate_recovery"
              with:
                recovery_strategy: "{{ .error.recovery_strategy }}"
                workflow_checkpoint: "{{ .workflow.checkpoint }}"
            false:
              type: signal
              signal_name: "manual_recovery_required"
              with:
                error_analysis: "{{ .error }}"
                workflow_state: "{{ .workflow }}"
```

This error handling pattern shows:
- **Multi-level error handling** with specific fallback strategies
- **Resource-aware recovery** with wait conditions
- **Graceful degradation** using different processing modes
- **Comprehensive logging** and notification systems
- **Recovery orchestration** with conditional retry logic

</Tab>
  <Tab value="State Management">
```yaml
# Advanced state management pattern
- id: stateful_workflow_pattern
  type: composite
  description: "Long-running workflow with persistent state management"
  
  tasks:
    # Initialize workflow state
    - id: initialize_state
      type: memory
      operation: store
      key: "workflow_state_{{ .workflow.id }}"
      value:
        phase: "initialization"
        progress: 0
        checkpoints: []
        metadata:
          started_at: "{{ now }}"
          total_items: "{{ len .workflow.input.items }}"
    
    # Phase 1: Data Preparation
    - id: preparation_phase
      type: composite
      tasks:
        - id: update_phase_state
          type: memory
          operation: update
          key: "workflow_state_{{ .workflow.id }}"
          update:
            phase: "preparation"
            progress: 10
        
        - id: prepare_data
          type: collection
          mode: sequential
          items: "{{ .workflow.input.items }}"
          task:
            id: "prepare_item_{{ .index }}"
            type: composite
            tasks:
              - id: process_item
                type: basic
                $use: tool(local::tools.#(id=="item_processor"))
                with:
                  item: "{{ .item }}"
                  phase: "preparation"
              
              - id: update_progress
                type: memory
                operation: update
                key: "workflow_state_{{ .workflow.id }}"
                update:
                  progress: "{{ add .workflow.state.progress 1 }}"
                  last_processed: "{{ .item.id }}"
              
              - id: create_checkpoint
                type: router
                condition: "{{ mod .index 10 }}"
                routes:
                  "0":
                    type: memory
                    operation: update
                    key: "workflow_state_{{ .workflow.id }}"
                    update:
                      checkpoints: "{{ append .workflow.state.checkpoints .index }}"
                      last_checkpoint: "{{ now }}"
    
    # Phase 2: Processing with State Recovery
    - id: processing_phase
      type: composite
      tasks:
        - id: load_current_state
          type: memory
          operation: load
          key: "workflow_state_{{ .workflow.id }}"
        
        - id: update_phase_state
          type: memory
          operation: update
          key: "workflow_state_{{ .workflow.id }}"
          update:
            phase: "processing"
            progress: 50
        
        - id: resume_from_checkpoint
          type: router
          condition: "{{ .tasks.load_current_state.output.last_checkpoint }}"
          routes:
            "exists":
              type: collection
              mode: parallel
              items: "{{ slice .workflow.input.items .tasks.load_current_state.output.last_processed }}"
              task:
                id: "process_item_{{ .index }}"
                type: basic
                $use: agent(local::agents.#(id=="item_processor"))
                action: process_item
                with:
                  item: "{{ .item }}"
                  state_context: "{{ .tasks.load_current_state.output }}"
            
            "default":
              type: collection
              mode: parallel
              items: "{{ .workflow.input.items }}"
              task:
                id: "process_item_{{ .index }}"
                type: basic
                $use: agent(local::agents.#(id=="item_processor"))
                action: process_item
                with:
                  item: "{{ .item }}"
    
    # Phase 3: Finalization
    - id: finalization_phase
      type: composite
      tasks:
        - id: update_final_state
          type: memory
          operation: update
          key: "workflow_state_{{ .workflow.id }}"
          update:
            phase: "finalization"
            progress: 90
            completed_at: "{{ now }}"
        
        - id: generate_summary
          type: basic
          $use: agent(local::agents.#(id=="summary_generator"))
          action: generate_summary
          with:
            workflow_state: "{{ .tasks.load_current_state.output }}"
            processing_results: "{{ .tasks.processing_phase.output }}"
        
        - id: cleanup_state
          type: memory
          operation: delete
          key: "workflow_state_{{ .workflow.id }}"
          delay: 7d  # Keep state for 7 days for debugging
```

This state management pattern demonstrates:

- **Persistent state storage** using memory tasks
- **Checkpoint-based recovery** for long-running workflows  
- **Progress tracking** with detailed metadata
- **Resume capabilities** from any checkpoint
- **Automatic cleanup** with configurable retention

</Tab>
</Tabs>

### 2. Event-Driven Architecture

**Build responsive, event-driven workflows using signals and wait tasks:**

<Tabs items={['Event Publisher', 'Event Subscriber', 'Event Coordination']}>
  <Tab value="Event Publisher">
```yaml
# Event publishing workflow
- id: event_publisher_workflow
  type: composite
  description: "Publish events to multiple subscribers with guaranteed delivery"
  
  tasks:
    # Prepare event data
    - id: prepare_event
      type: basic
      $use: tool(local::tools.#(id=="event_serializer"))
      with:
        event_type: "{{ .workflow.input.event_type }}"
        payload: "{{ .workflow.input.payload }}"
        metadata:
          source: "{{ .workflow.input.source }}"
          timestamp: "{{ now }}"
          trace_id: "{{ .workflow.id }}"
    
    # Determine subscribers
    - id: resolve_subscribers
      type: basic
      $use: tool(local::tools.#(id=="subscriber_resolver"))
      with:
        event_type: "{{ .workflow.input.event_type }}"
        routing_rules: "{{ .workflow.input.routing_rules }}"
    
    # Publish to all subscribers
    - id: publish_events
      type: collection
      mode: parallel
      max_workers: 10
      items: "{{ .tasks.resolve_subscribers.output.subscribers }}"
      task:
        id: "publish_to_{{ .item.id }}"
        type: composite
        tasks:
          # Store event for delivery guarantee
          - id: store_event
            type: memory
            operation: store
            key: "event_{{ .workflow.id }}_{{ .item.id }}"
            value:
              event: "{{ .tasks.prepare_event.output }}"
              subscriber: "{{ .item }}"
              status: "pending"
              attempts: 0
              max_attempts: 3
            ttl: 24h
          
          # Publish event with retry
          - id: publish_with_retry
            type: basic
            $use: tool(local::tools.#(id=="event_publisher"))
            with:
              event: "{{ .tasks.prepare_event.output }}"
              subscriber: "{{ .item }}"
              delivery_guarantee: "at_least_once"
            on_error:
              next: retry_publish
          
          # Retry logic
          - id: retry_publish
            type: router
            condition: "{{ .tasks.store_event.output.attempts }}"
            routes:
              "lt 3":
                type: wait
                condition: "{{ mul .tasks.store_event.output.attempts 30 }}s"
                processor:
                  type: basic
                  $use: tool(local::tools.#(id=="event_publisher"))
                  with:
                    event: "{{ .tasks.prepare_event.output }}"
                    subscriber: "{{ .item }}"
                    attempt: "{{ add .tasks.store_event.output.attempts 1 }}"
              
              "default":
                type: signal
                signal_name: "event_delivery_failed"
                with:
                  event_id: "{{ .tasks.prepare_event.output.id }}"
                  subscriber: "{{ .item }}"
                  final_error: "{{ .error }}"
          
          # Mark as delivered
          - id: mark_delivered
            type: memory
            operation: update
            key: "event_{{ .workflow.id }}_{{ .item.id }}"
            update:
              status: "delivered"
              delivered_at: "{{ now }}"
    
    # Publish completion event
    - id: publish_completion
      type: signal
      signal_name: "event_publication_complete"
      with:
        event_id: "{{ .tasks.prepare_event.output.id }}"
        total_subscribers: "{{ len .tasks.resolve_subscribers.output.subscribers }}"
        successful_deliveries: "{{ len .tasks.publish_events.output.successful }}"
        failed_deliveries: "{{ len .tasks.publish_events.output.failed }}"
```

This event publisher shows:

- **Guaranteed delivery** with retry logic and state persistence
- **Parallel publishing** for performance
- **Failure tracking** and notification
- **Event completion** signaling

</Tab>
  <Tab value="Event Subscriber">
```yaml
# Event subscriber workflow
- id: event_subscriber_workflow
  type: composite
  description: "Subscribe to and process events with idempotency"
  
  tasks:
    # Wait for events
    - id: wait_for_events
      type: wait
      condition: "event_received"
      timeout: 30m
      processor:
        type: composite
        tasks:
          # Check for duplicate processing
          - id: check_idempotency
            type: memory
            operation: load
            key: "processed_event_{{ .signal.event_id }}"
          
          # Process if not already processed
          - id: process_event
            type: router
            condition: "{{ .tasks.check_idempotency.output.exists }}"
            routes:
              false:
                type: composite
                tasks:
                  # Mark as processing
                  - id: mark_processing
                    type: memory
                    operation: store
                    key: "processed_event_{{ .signal.event_id }}"
                    value:
                      status: "processing"
                      started_at: "{{ now }}"
                      processor_id: "{{ .workflow.id }}"
                    ttl: 1h
                  
                  # Validate event
                  - id: validate_event
                    type: basic
                    $use: tool(local::tools.#(id=="event_validator"))
                    with:
                      event: "{{ .signal.event_data }}"
                      validation_schema: "{{ .workflow.input.schema }}"
                  
                  # Route by event type
                  - id: route_by_type
                    type: router
                    condition: "{{ .signal.event_data.type }}"
                    routes:
                      "user_created":
                        type: basic
                        $use: agent(local::agents.#(id=="user_handler"))
                        action: handle_user_created
                        with:
                          event: "{{ .signal.event_data }}"
                      
                      "order_placed":
                        type: parallel
                        tasks:
                          - id: update_inventory
                            type: basic
                            $use: tool(local::tools.#(id=="inventory_manager"))
                            with:
                              order_data: "{{ .signal.event_data.payload }}"
                          
                          - id: send_confirmation
                            type: basic
                            $use: tool(local::tools.#(id=="notification_sender"))
                            with:
                              recipient: "{{ .signal.event_data.payload.customer_email }}"
                              template: "order_confirmation"
                              data: "{{ .signal.event_data.payload }}"
                      
                      "payment_processed":
                        type: basic
                        $use: tool(local::tools.#(id=="payment_processor"))
                        with:
                          payment_data: "{{ .signal.event_data.payload }}"
                      
                      "default":
                        type: signal
                        signal_name: "unknown_event_type"
                        with:
                          event: "{{ .signal.event_data }}"
                          processor: "{{ .workflow.id }}"
                  
                  # Mark as completed
                  - id: mark_completed
                    type: memory
                    operation: update
                    key: "processed_event_{{ .signal.event_id }}"
                    update:
                      status: "completed"
                      completed_at: "{{ now }}"
                      result: "{{ .tasks.route_by_type.output }}"
              
              true:
                type: basic
                $use: tool(local::tools.#(id=="logger"))
                with:
                  level: "info"
                  message: "Event {{ .signal.event_id }} already processed"
    
    # Continue waiting for more events
    - id: continue_listening
      type: router
      condition: "{{ .workflow.input.continuous_mode }}"
      routes:
        true:
          type: signal
          signal_name: "restart_listener"
          with:
            subscriber_id: "{{ .workflow.id }}"
```

This event subscriber demonstrates:

- **Idempotency** with duplicate detection
- **Event validation** and routing
- **Parallel processing** for complex events
- **Continuous operation** mode

</Tab>
  <Tab value="Event Coordination">
```yaml
# Event coordination workflow
- id: event_coordination_workflow
  type: composite
  description: "Coordinate multiple events to trigger complex business processes"
  
  tasks:
    # Initialize coordination state
    - id: initialize_coordination
      type: memory
      operation: store
      key: "coordination_{{ .workflow.id }}"
      value:
        required_events: "{{ .workflow.input.required_events }}"
        received_events: []
        coordination_timeout: "{{ .workflow.input.timeout }}"
        started_at: "{{ now }}"
    
    # Wait for multiple events with timeout
    - id: coordinate_events
      type: parallel
      strategy: race
      tasks:
        # Event collection task
        - id: collect_events
          type: composite
          tasks:
            - id: wait_for_event_1
              type: wait
              condition: "{{ .workflow.input.required_events[0] }}"
              timeout: "{{ .workflow.input.timeout }}"
              processor:
                type: memory
                operation: update
                key: "coordination_{{ .workflow.id }}"
                update:
                  received_events: "{{ append .state.received_events .signal }}"
            
            - id: wait_for_event_2
              type: wait
              condition: "{{ .workflow.input.required_events[1] }}"
              timeout: "{{ .workflow.input.timeout }}"
              processor:
                type: memory
                operation: update
                key: "coordination_{{ .workflow.id }}"
                update:
                  received_events: "{{ append .state.received_events .signal }}"
            
            - id: wait_for_event_3
              type: wait
              condition: "{{ .workflow.input.required_events[2] }}"
              timeout: "{{ .workflow.input.timeout }}"
              processor:
                type: memory
                operation: update
                key: "coordination_{{ .workflow.id }}"
                update:
                  received_events: "{{ append .state.received_events .signal }}"
            
            # Check completion condition
            - id: check_completion
              type: router
              condition: "{{ len .state.received_events }}"
              routes:
                "eq {{ len .workflow.input.required_events }}":
                  type: signal
                  signal_name: "coordination_complete"
                  with:
                    events: "{{ .state.received_events }}"
                    coordination_id: "{{ .workflow.id }}"
        
        # Timeout task
        - id: timeout_handler
          type: wait
          condition: "{{ .workflow.input.timeout }}"
          processor:
            type: signal
            signal_name: "coordination_timeout"
            with:
              coordination_id: "{{ .workflow.id }}"
              received_events: "{{ .state.received_events }}"
              missing_events: "{{ .workflow.input.required_events }}"
    
    # Process coordinated events
    - id: process_coordination_result
      type: router
      condition: "{{ .tasks.coordinate_events.output.winner }}"
      routes:
        "collect_events":
          type: composite
          tasks:
            - id: validate_event_sequence
              type: basic
              $use: tool(local::tools.#(id=="sequence_validator"))
              with:
                events: "{{ .tasks.coordinate_events.output.events }}"
                sequence_rules: "{{ .workflow.input.sequence_rules }}"
            
            - id: execute_business_process
              type: basic
              $use: agent(local::agents.#(id=="business_process_executor"))
              action: execute_coordinated_process
              with:
                coordinated_events: "{{ .tasks.coordinate_events.output.events }}"
                business_context: "{{ .workflow.input.business_context }}"
            
            - id: notify_completion
              type: signal
              signal_name: "business_process_completed"
              with:
                process_id: "{{ .tasks.execute_business_process.output.process_id }}"
                result: "{{ .tasks.execute_business_process.output }}"
        
        "timeout_handler":
          type: composite
          tasks:
            - id: handle_partial_events
              type: basic
              $use: agent(local::agents.#(id=="partial_event_handler"))
              action: handle_partial_coordination
              with:
                received_events: "{{ .tasks.coordinate_events.output.received_events }}"
                missing_events: "{{ .tasks.coordinate_events.output.missing_events }}"
            
            - id: notify_timeout
              type: signal
              signal_name: "coordination_timeout_handled"
              with:
                coordination_id: "{{ .workflow.id }}"
                partial_result: "{{ .tasks.handle_partial_events.output }}"
    
    # Cleanup coordination state
    - id: cleanup_coordination
      type: memory
      operation: delete
      key: "coordination_{{ .workflow.id }}"
```

This coordination pattern shows:
- **Multi-event coordination** with timeout handling
- **Race conditions** between completion and timeout
- **Partial event processing** for timeout scenarios
- **State cleanup** after coordination completion

</Tab>
</Tabs>

### 3. Resource Management and Optimization

**Optimize resource usage and performance with intelligent task scheduling:**

```yaml
# Resource-aware workflow optimization
- id: resource_optimized_workflow
  type: composite
  description: "Intelligent resource management and optimization"
  
  tasks:
    # Resource assessment
    - id: assess_resources
      type: basic
      $use: tool(local::tools.#(id=="resource_monitor"))
      with:
        metrics: ["cpu", "memory", "disk", "network"]
        threshold_config: "{{ .workflow.input.resource_thresholds }}"
    
    # Adaptive task scheduling
    - id: schedule_tasks
      type: router
      condition: "{{ .tasks.assess_resources.output.resource_availability }}"
      routes:
        "high":
          type: parallel
          strategy: wait_all
          max_workers: 16
          tasks:
            - id: cpu_intensive_batch
              type: collection
              mode: parallel
              max_workers: 8
              items: "{{ .workflow.input.cpu_intensive_tasks }}"
              task:
                id: "cpu_task_{{ .index }}"
                type: basic
                $use: tool(local::tools.#(id=="cpu_processor"))
                with:
                  task_data: "{{ .item }}"
                  optimization_level: "aggressive"
            
            - id: memory_intensive_batch
              type: collection
              mode: parallel
              max_workers: 4
              items: "{{ .workflow.input.memory_intensive_tasks }}"
              task:
                id: "memory_task_{{ .index }}"
                type: basic
                $use: tool(local::tools.#(id=="memory_processor"))
                with:
                  task_data: "{{ .item }}"
                  memory_allocation: "high"
        
        "medium":
          type: parallel
          strategy: wait_all
          max_workers: 8
          tasks:
            - id: balanced_processing
              type: collection
              mode: sequential
              items: "{{ .workflow.input.all_tasks }}"
              task:
                id: "balanced_task_{{ .index }}"
                type: basic
                $use: tool(local::tools.#(id=="balanced_processor"))
                with:
                  task_data: "{{ .item }}"
                  optimization_level: "balanced"
        
        "low":
          type: collection
          mode: sequential
          items: "{{ .workflow.input.all_tasks }}"
          task:
            id: "conservative_task_{{ .index }}"
            type: composite
            tasks:
              - id: wait_for_resources
                type: wait
                condition: "{{ .system.resources.available }}"
                max_wait: 5m
                processor:
                  type: basic
                  $use: tool(local::tools.#(id=="conservative_processor"))
                  with:
                    task_data: "{{ .item }}"
                    optimization_level: "conservative"
    
    # Dynamic load balancing
    - id: load_balancing
      type: router
      condition: "{{ .system.load.current }}"
      routes:
        "gt 0.8":
          type: signal
          signal_name: "scale_out_request"
          with:
            current_load: "{{ .system.load.current }}"
            workflow_id: "{{ .workflow.id }}"
        
        "lt 0.3":
          type: signal
          signal_name: "scale_in_request"
          with:
            current_load: "{{ .system.load.current }}"
            workflow_id: "{{ .workflow.id }}"
    
    # Resource cleanup
    - id: resource_cleanup
      type: parallel
      tasks:
        - id: clean_temporary_files
          type: basic
          $use: tool(local::tools.#(id=="file_cleaner"))
          with:
            directories: "{{ .workflow.temp_directories }}"
            max_age: "1h"
        
        - id: release_connections
          type: basic
          $use: tool(local::tools.#(id=="connection_manager"))
          with:
            action: "release_idle"
            idle_timeout: "5m"
        
        - id: clear_caches
          type: basic
          $use: tool(local::tools.#(id=="cache_manager"))
          with:
            action: "clear_expired"
            workflow_id: "{{ .workflow.id }}"
```

This resource optimization pattern demonstrates:
- **Dynamic resource assessment** and adaptive scheduling
- **Load-based task distribution** with different optimization levels
- **Automatic scaling** triggers based on system load
- **Comprehensive cleanup** of temporary resources

## Best Practices for Advanced Patterns

### 1. Pattern Composition Guidelines

**Combine patterns effectively for maximum benefit:**

<Steps>
<Step title="Layer Patterns Appropriately" description="Stack patterns from foundational to specialized">

```yaml
# Layered pattern composition
- id: enterprise_workflow
  type: composite
  tasks:
    # Foundation layer: Resource management
    - id: resource_layer
      type: basic
      $use: tool(local::tools.#(id=="resource_allocator"))
    
    # Processing layer: Multi-stage pipeline
    - id: processing_layer
      type: composite
      tasks:
        # ... multi-stage processing tasks
    
    # Coordination layer: Event-driven coordination
    - id: coordination_layer
      type: parallel
      tasks:
        # ... event coordination tasks
    
    # Optimization layer: Performance tuning
    - id: optimization_layer
      type: router
      condition: "{{ .performance.metrics }}"
      routes:
        # ... optimization routes
```

</Step>

<Step title="Implement Proper Error Boundaries" description="Isolate failures and enable recovery">

```yaml
# Error boundary pattern
- id: error_boundary_example
  type: composite
  tasks:
    - id: critical_section
      type: composite
      on_error:
        next: critical_error_handler
      tasks:
        # ... critical tasks
    
    - id: non_critical_section
      type: parallel
      strategy: best_effort
      tasks:
        # ... non-critical tasks that can fail
```

</Step>

<Step title="Use Appropriate Concurrency Models" description="Match concurrency to workload characteristics">

```yaml
# Concurrency model selection
- id: concurrency_patterns
  type: router
  condition: "{{ .workload.type }}"
  routes:
    "cpu_bound":
      type: parallel
      max_workers: "{{ .system.cpu_cores }}"
    
    "io_bound":
      type: parallel
      max_workers: "{{ mul .system.cpu_cores 2 }}"
    
    "mixed":
      type: composite
      tasks:
        - id: cpu_phase
          type: parallel
          max_workers: "{{ .system.cpu_cores }}"
        - id: io_phase
          type: parallel
          max_workers: "{{ mul .system.cpu_cores 2 }}"
```

</Step>
</Steps>

### 2. Performance Optimization

**Optimize workflow performance with smart design choices:**

- **Use parallel tasks** for independent operations
- **Implement caching** with memory tasks for expensive computations
- **Apply resource limits** to prevent resource exhaustion
- **Monitor and adjust** based on performance metrics
- **Use appropriate batch sizes** for collection processing

### 3. Reliability and Resilience

**Build robust workflows that handle failures gracefully:**

- **Implement circuit breakers** with router tasks and failure counting
- **Use timeout patterns** to prevent hung workflows
- **Design idempotent operations** to enable safe retries
- **Create comprehensive logging** for troubleshooting
- **Plan for partial failures** in distributed operations

## Next Steps

<Steps>
<Step title="Study Real-World Examples" description="Explore practical implementations">

- **[Business Workflow Examples](/docs/core/examples/business-workflows)** - Production patterns
- **[Integration Patterns](/docs/core/examples/integration-patterns)** - External service integration
- **[Performance Benchmarks](/docs/core/examples/performance-benchmarks)** - Optimization techniques

</Step>

<Step title="Master Individual Components" description="Deep dive into specific task types">

- **[Signal Communication](/docs/core/signals/signal-overview)** - Event-driven architecture
- **[Memory Management](/docs/core/tasks/memory-tasks)** - State persistence
- **[Error Handling](/docs/core/tasks/basic-tasks#error-handling)** - Failure recovery

</Step>

<Step title="Build Your Own Patterns" description="Create custom patterns for your use cases">

- **[Workflow Design Principles](/docs/core/workflow-design/principles)** - Design guidelines
- **[Testing Strategies](/docs/core/development/testing-strategies)** - Validation approaches
- **[Monitoring and Observability](/docs/core/monitoring/overview)** - Production readiness

</Step>
</Steps>