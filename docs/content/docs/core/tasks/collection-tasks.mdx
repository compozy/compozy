---
title: Collection Tasks
description: Collection tasks provide powerful iteration patterns for processing arrays and collections in Compozy workflows. They transform array data into parallel or sequential task executions, enabling efficient batch processing with sophisticated filtering, error handling, and result aggregation capabilities.
---

## Overview

Collection tasks enable sophisticated array processing with enterprise-grade capabilities. They automatically transform input arrays into individual task executions, providing powerful orchestration patterns for batch operations.

<Mermaid chart={`graph LR
    A[Input Array] --> B{Collection Task}
    B --> C[Filter Items]
    C --> D{Processing Mode}
    D -->|Sequential| E[Process One by One]
    D -->|Parallel| F[Process Concurrently]
    E --> G[Collect Results]
    F --> G[Collect Results]
    G --> H[Aggregated Output]

    style A fill:#101010,color:#fff
    style B fill:#101010,color:#fff
    style C fill:#101010,color:#fff
    style D fill:#101010,color:#fff
    style E fill:#101010,color:#fff
    style F fill:#101010,color:#fff
    style G fill:#101010,color:#fff
    style H fill:#101010,color:#fff`} />

### Key Capabilities

<FeatureCardList cols={2} size="sm">
  <FeatureCard
    title="Intelligent Processing Modes"
    description="Choose between sequential ordering and parallel performance based on your workflow requirements"
    icon="GitBranch"
  />
  <FeatureCard
    title="Advanced Filtering"
    description="Use CEL expressions to process only items that meet specific conditions, reducing unnecessary processing"
    icon="Filter"
  />
  <FeatureCard
    title="Batch Optimization"
    description="Process large datasets efficiently with configurable batch sizes and memory management"
    icon="Package"
  />
  <FeatureCard
    title="Rich Context Access"
    description="Access current item, processing index, collection metadata, and parent workflow context"
    icon="Database"
  />
  <FeatureCard
    title="Failure Resilience"
    description="Sophisticated error handling with partial results, best-effort strategies, and graceful degradation"
    icon="Shield"
  />
  <FeatureCard
    title="Result Aggregation"
    description="Automatically collect and organize results from all iterations with template-based transformations"
    icon="BarChart"
  />
</FeatureCardList>

<Callout type="info">
Collection tasks are essential for data processing pipelines, user batch operations, file transformations, and any scenario requiring the same operation across multiple items. They bridge the gap between single-item processing and large-scale batch operations.
</Callout>

## Task Structure

### Basic Collection Task

```yaml
id: process-users
type: collection
mode: parallel
strategy: best_effort

# Source collection
items: "{{ .workflow.input.users }}"

# Optional filtering
filter: "{{ ne .item.status 'inactive' }}"

# Task template applied to each item
task:
  id: "process-user-{{ .index }}"
  agent: user-processor
  action: process_user
  with:
    user_id: "{{ .item.id }}"
    user_data: "{{ .item }}"
    processing_index: "{{ .index }}"

outputs:
  processed_users: "{{ .output }}"
  total_processed: "{{ len .output }}"
```

### Configuration Options

<Tabs items={["Execution Modes", "Filtering", "Batching", "Variables"]}>
<Tab value="Execution Modes">

Control how collection items are processed:

```yaml title="Sequential Processing"
id: sequential-collection
type: collection
mode: sequential
items: "{{ .workflow.input.documents }}"

task:
  id: "process-doc-{{ .index }}"
  tool: document-processor
  with:
    document: "{{ .item }}"
    sequence_number: "{{ .index }}"
```

```yaml title="Parallel Processing"
id: parallel-collection
type: collection
mode: parallel
strategy: wait_all
max_workers: 8
items: "{{ .workflow.input.images }}"

task:
  id: "process-image-{{ .index }}"
  tool: image-processor
  with:
    image: "{{ .item }}"
    parallel_index: "{{ .index }}"
```

```yaml title="Batched Processing"
id: batched-collection
type: collection
mode: parallel
batch_size: 5
items: "{{ .workflow.input.records }}"

task:
  id: "process-batch-{{ .batch_index }}"
  tool: batch-processor
  with:
    records: "{{ .batch }}"
    batch_number: "{{ .batch_index }}"
```

</Tab>
<Tab value="Filtering">

Filter items before processing:

```yaml title="Filter by Status"
id: filter-by-status
type: collection
items: "{{ .workflow.input.orders }}"
filter: "{{ eq .item.status 'pending' }}"

task:
  id: "process-order-{{ .index }}"
  agent: order-processor
  action: process_order
  with:
    order: "{{ .item }}"
```

```yaml title="Complex Filtering"
id: complex-filter
type: collection
items: "{{ .workflow.input.users }}"
filter: "{{ and (ne .item.status 'inactive') (gt .item.age 18) (ne .item.email '') }}"

task:
  id: "process-user-{{ .index }}"
  agent: user-processor
  action: process_adult_user
  with:
    user: "{{ .item }}"
```

```yaml title="Date-based Filtering"
id: date-filter
type: collection
items: "{{ .workflow.input.events }}"
filter: "{{ and (gt .item.date '2024-01-01') (lt .item.date '2024-12-31') }}"

task:
  id: "process-event-{{ .index }}"
  tool: event-processor
  with:
    event: "{{ .item }}"
    year: "2024"
```

</Tab>
<Tab value="Batching">

Process items in batches for efficiency:

```yaml title="Fixed Batch Size"
id: fixed-batch
type: collection
mode: parallel
batch_size: 10
items: "{{ .workflow.input.large_dataset }}"

task:
  id: "process-batch-{{ .batch_index }}"
  tool: batch-processor
  with:
    batch: "{{ .batch }}"
    batch_size: "{{ len .batch }}"
    batch_number: "{{ .batch_index }}"
```

```yaml title="Dynamic Batch Sizing"
id: dynamic-batch
type: collection
mode: parallel
batch_size: "{{ .env.BATCH_SIZE | default 5 | toNumber }}"
items: "{{ .workflow.input.items }}"

task:
  id: "process-batch-{{ .batch_index }}"
  agent: batch-processor
  action: process_batch
  with:
    items: "{{ .batch }}"
    batch_meta:
      size: "{{ len .batch }}"
      index: "{{ .batch_index }}"
      total_batches: "{{ .total_batches }}"
```

```yaml title="Batch with Overlap"
id: overlapping-batch
type: collection
mode: sequential
batch_size: 8
batch_overlap: 2
items: "{{ .workflow.input.time_series }}"

task:
  id: "analyze-window-{{ .batch_index }}"
  agent: time-series-analyzer
  action: analyze_window
  with:
    window_data: "{{ .batch }}"
    window_size: "{{ len .batch }}"
    overlap: 2
```

</Tab>
<Tab value="Variables">

Use collection-specific variables in tasks:

```yaml title="Collection Variables"
id: collection-variables
type: collection
items: "{{ .workflow.input.products }}"
item_var: product     # Custom variable name for current item
index_var: idx        # Custom variable name for current index

task:
  id: "process-product-{{ .idx }}"
  agent: product-processor
  action: process_product
  with:
    # Access current item
    product_id: "{{ .product.id }}"
    product_name: "{{ .product.name }}"
    product_price: "{{ .product.price }}"

    # Access index information
    position: "{{ .idx }}"
    is_first: "{{ eq .idx 0 }}"
    is_last: "{{ eq .idx (sub (len .collection) 1) }}"

    # Access collection metadata
    total_items: "{{ len .collection }}"
    collection_size: "{{ .collection_size }}"

    # Access parent context
    workflow_id: "{{ .workflow.id }}"
    processing_mode: "{{ .workflow.input.mode }}"
```

```yaml title="Default Variables"
id: default-variables
type: collection
items: "{{ .workflow.input.tasks }}"

task:
  id: "execute-task-{{ .index }}"
  tool: task-executor
  with:
    task_definition: "{{ .item }}"
    execution_order: "{{ .index }}"
    total_tasks: "{{ len .collection }}"
```

</Tab>
</Tabs>

## Processing Patterns

<Callout type="info">
Understanding when to use each processing pattern is crucial for optimal performance. Learn more about advanced patterns and template expressions to make the best choice for your use case.
</Callout>

### Sequential Processing

Process items one after another when order matters or when each task depends on previous results:

**When to Use Sequential Processing:**
- Order-dependent operations (document processing, data transformations)
- Resource-constrained environments with limited parallel capacity
- Operations that build on previous results
- Rate-limited external APIs that require sequential calls

### Parallel Processing

Process items concurrently when order doesn't matter and you need maximum throughput:

**When to Use Parallel Processing:**
- Independent operations that don't depend on each other
- CPU or I/O intensive tasks that benefit from concurrency
- Large datasets where processing time is a constraint
- Multiple API calls that can be made simultaneously

**Performance Considerations:**
- Use `max_workers` to control resource usage and prevent overwhelming external services
- Consider memory usage when processing large items in parallel
- Monitor external API rate limits and adjust concurrency accordingly

### Batch Processing

Process large datasets in manageable chunks to optimize memory usage and provide better control over resource consumption:

**When to Use Batch Processing:**
- Processing thousands or millions of items
- Memory-constrained environments
- External APIs with bulk operation support
- Database operations that benefit from batch inserts/updates

**Batch Size Guidelines:**
- **Small batches (1-10)**: Real-time processing, low latency requirements
- **Medium batches (10-100)**: Balanced performance and resource usage
- **Large batches (100-1000+)**: Maximum throughput for bulk operations

Learn more about deployment options and advanced patterns.

## Advanced Features

<Callout type="warning">
Advanced collection features require a solid understanding of [CEL expressions](/docs/core/yaml-templates/context-variables) and [template context variables](/docs/core/yaml-templates/context-variables). Review these concepts before implementing complex filtering and error handling patterns.
</Callout>

### Conditional Processing

<AccordionGroup>
<Accordion title="Advanced Filtering with CEL Expressions">

Implement sophisticated filtering logic using CEL expressions for complex business rules:

```yaml
id: conditional-processing
type: collection
items: "{{ .workflow.input.transactions }}"

# Complex filtering with multiple conditions
filter: |
  {{
    and
    (gt .item.amount 100)
    (or
      (eq .item.type 'purchase')
      (eq .item.type 'refund')
    )
    (not (contains .item.tags 'test'))
  }}

task:
  id: "process-transaction-{{ .index }}"
  agent: transaction-processor
  action: process_transaction
  with:
    transaction: "{{ .item }}"
    special_handling: "{{ gt .item.amount 1000 }}"
    risk_level: |
      {{
        if (gt .item.amount 10000) "high"
        else if (gt .item.amount 1000) "medium"
        else "low"
      }}
```

**Common Filtering Patterns:**

<List>
  <ListItem title="Numeric Ranges" icon="Hash">
    `{{ and (gt .item.price 10) (lt .item.price 100) }}` - Filter by price range
  </ListItem>
  <ListItem title="String Matching" icon="Type">
    `{{ contains .item.email "@company.com" }}` - Domain-specific filtering
  </ListItem>
  <ListItem title="Date Filtering" icon="Calendar">
    `{{ gt .item.created_at "2024-01-01T00:00:00Z" }}` - Recent items only
  </ListItem>
  <ListItem title="Status Checking" icon="CheckCircle">
    `{{ in .item.status ["active", "pending"] }}` - Multiple valid statuses
  </ListItem>
  <ListItem title="Complex Logic" icon="GitBranch">
    `{{ and (eq .item.type "premium") (gt .item.score 85) }}` - Multi-criteria filtering
  </ListItem>
</List>

</Accordion>

<Accordion title="Nested Collections">

Process collections within collections:

```yaml
id: nested-collection-processing
type: collection
items: "{{ .workflow.input.departments }}"

task:
  id: "process-department-{{ .index }}"
  mode: parallel
  items: "{{ .item.employees }}"

  task:
    id: "process-employee-{{ .parent_index }}-{{ .index }}"
    type: basic
    agent: employee-processor
    action: process_employee
    with:
      employee: "{{ .item }}"
      department: "{{ .parent_item.name }}"
      department_index: "{{ .parent_index }}"
      employee_index: "{{ .index }}"

  outputs:
    department_name: "{{ .item.name }}"
    processed_employees: "{{ .output }}"
    employee_count: "{{ len .output }}"

outputs:
  all_departments: "{{ .output }}"
  total_employees: "{{ sum (map .output 'employee_count') }}"
  department_summaries: "{{ map .output (dict 'name' .department_name 'count' .employee_count) }}"
```

</Accordion>
</AccordionGroup>

## Best Practices

<FeatureCardList cols={2} size="sm">
  <FeatureCard
    title="Choose the Right Processing Mode"
    description="Use sequential for order-dependent processing, parallel for independent operations, and batch for large datasets"
    icon="GitBranch"
  />
  <FeatureCard
    title="Implement Early Filtering"
    description="Filter items before processing to reduce load and improve performance using CEL expressions"
    icon="Filter"
  />
  <FeatureCard
    title="Handle Partial Failures Gracefully"
    description="Use best_effort strategy for non-critical operations and implement proper error handling"
    icon="Shield"
  />
  <FeatureCard
    title="Optimize Batch Configurations"
    description="Balance memory usage with processing efficiency by choosing appropriate batch sizes"
    icon="Gauge"
  />
  <FeatureCard
    title="Monitor Performance Metrics"
    description="Track processing times, success rates, and resource usage to optimize collection performance"
    icon="BarChart3"
  />
  <FeatureCard
    title="Set Realistic Timeouts"
    description="Configure appropriate timeouts for individual items and overall collection operations"
    icon="Clock"
  />
</FeatureCardList>

## Performance Guidelines

<Callout type="warning" className="mt-4 mb-8" title="Performance Optimization">
For production deployments, consider implementing [monitoring and observability](/docs/core/configuration/monitoring) to track collection task performance and identify optimization opportunities.
</Callout>

### Memory Management

<List icon="Check" size="md" className="mt-4">
  <ListItem>
    Use batch processing for datasets larger than 1000 items
  </ListItem>
  <ListItem>
    Monitor memory usage in production environments
  </ListItem>
  <ListItem>
    Consider streaming patterns for extremely large datasets
  </ListItem>
</List>

### Concurrency Control

<List icon="Check" size="md" className="mt-4">
  <ListItem>
    Set `max_workers` based on external service limits
  </ListItem>
  <ListItem>
    Use sequential mode for rate-limited APIs
  </ListItem>
  <ListItem>
    Implement backoff strategies for failed requests
  </ListItem>
</List>

### Error Recovery

<List icon="Check" size="md" className="mt-4">
  <ListItem>
    Always use `best_effort` for non-critical operations
  </ListItem>
  <ListItem>
    Implement proper logging for failed items
  </ListItem>
  <ListItem>
    Consider retry mechanisms for transient failures
  </ListItem>
</List>

## Next Steps

<Tabs items={["Beginner", "Intermediate", "Advanced", "Production"]}>
<Tab>

<ReferenceCardList>
  <ReferenceCard
    title="Basic Tasks Foundation"
    description="Master the foundation tasks used within collection task templates"
    href="/docs/core/tasks/basic-tasks"
    icon="Package"
  />
  <ReferenceCard
    title="First Workflow Tutorial"
    description="Build your first workflow with collection processing examples"
    href="/docs/core/getting-started/first-workflow"
    icon="Rocket"
  />
  <ReferenceCard
    title="Template Basics"
    description="Learn template expressions for dynamic collection configuration"
    href="/docs/core/yaml-templates/overview"
    icon="FileCode"
  />
  <ReferenceCard
    title="Agent Integration"
    description="Connect AI agents to process collection items intelligently"
    href="/docs/core/agents/overview"
    icon="BrainCircuit"
  />
</ReferenceCardList>

</Tab>
<Tab>

<ReferenceCardList>
  <ReferenceCard
    title="Parallel Processing Strategies"
    description="Learn parallel execution strategies and performance optimization"
    href="/docs/core/tasks/parallel-processing"
    icon="Zap"
  />
  <ReferenceCard
    title="Aggregate Tasks"
    description="Combine and transform collection results into unified outputs"
    href="/docs/core/tasks/aggregate-tasks"
    icon="BarChart"
  />
  <ReferenceCard
    title="Advanced Template Functions"
    description="Master complex template expressions and data manipulation"
    href="/docs/core/yaml-templates/sprig-functions"
    icon="Code"
  />
  <ReferenceCard
    title="Error Handling Patterns"
    description="Implement robust error handling and recovery strategies"
    href="/docs/core/tasks/basic-tasks#error-handling"
    icon="Shield"
  />
</ReferenceCardList>

</Tab>
<Tab>

<ReferenceCardList>
  <ReferenceCard
    title="Flow Control & Routing"
    description="Implement conditional logic and routing in collection workflows"
    href="/docs/core/tasks/composite-tasks"
    icon="GitBranch"
  />
  <ReferenceCard
    title="Memory & State Management"
    description="Add persistent state and memory to collection workflows"
    href="/docs/core/memory/overview"
    icon="Database"
  />
  <ReferenceCard
    title="Signal Coordination"
    description="Coordinate complex multi-collection workflows with signals"
    href="/docs/core/signals/overview"
    icon="Radio"
  />
  <ReferenceCard
    title="Custom Tool Development"
    description="Build specialized tools for collection item processing"
    href="/docs/core/tools/overview"
    icon="Wrench"
  />
</ReferenceCardList>

</Tab>
<Tab>

<ReferenceCardList>
  <ReferenceCard
    title="Performance Optimization"
    description="Optimize collection tasks for high-volume production workloads"
    href="/docs/core/tasks/parallel-processing"
    icon="Gauge"
  />
  <ReferenceCard
    title="Monitoring & Observability"
    description="Monitor collection task performance and debug issues"
    href="/docs/core/configuration/monitoring"
    icon="BarChart3"
  />
  <ReferenceCard
    title="Scaling Strategies"
    description="Scale collection processing for enterprise-grade deployments"
    href="/docs/core/deployment/kubernetes"
    icon="TrendingUp"
  />
  <ReferenceCard
    title="Security Best Practices"
    description="Secure collection workflows and handle sensitive data"
    href="/docs/core/tools/runtime-environment"
    icon="Lock"
  />
</ReferenceCardList>

</Tab>
</Tabs>

---

Collection tasks are the backbone of batch processing in Compozy, transforming arrays into sophisticated workflows with enterprise-grade reliability, performance, and flexibility. Master these patterns to build scalable data processing solutions that grow with your needs.
