---
title: "LLM Integration"
description: "How agents integrate with various LLM providers through a unified interface"
---

## Overview

Compozy agents integrate with multiple LLM providers through a unified interface, allowing seamless switching between models and providers. This abstraction enables you to choose the best model for each task while maintaining consistent configuration patterns.

<Callout type="info">
**Schema Reference**: For the most up-to-date configuration schema and properties, see the [Provider Schema Documentation](/docs/schema/provider).
</Callout>

## Supported Providers

<FeatureCardList cols={3}>
  <FeatureCard
    title="OpenAI"
    description="Access GPT-4, GPT-3.5, and other OpenAI models with full function calling support"
    icon="Cpu"
  />
  <FeatureCard
    title="Anthropic"
    description="Claude 3 family (Opus, Sonnet, Haiku) with advanced reasoning capabilities"
    icon="Bot"
  />
  <FeatureCard
    title="Google"
    description="Gemini Pro, Gemini Ultra, and PaLM models for multimodal applications"
    icon="Globe"
  />
  <FeatureCard
    title="Groq"
    description="Fast inference with Llama, Mixtral, and other open models at low latency"
    icon="Microchip"
  />
  <FeatureCard
    title="Ollama"
    description="Run any Ollama-supported model locally for privacy-first applications"
    icon="Server"
  />
  <FeatureCard
    title="DeepSeek"
    description="DeepSeek AI models with OpenAI-compatible API for advanced reasoning"
    icon="Zap"
  />
  <FeatureCard
    title="xAI"
    description="Grok models with OpenAI-compatible API for cutting-edge AI capabilities"
    icon="Sparkles"
  />
  <FeatureCard
    title="Custom"
    description="Connect to any OpenAI-compatible API endpoint for specialized models"
    icon="Plug"
  />
</FeatureCardList>

## Provider Configuration

### Basic Configuration

Each agent requires a provider configuration that specifies the LLM to use. The `model` property accepts any valid model string for the chosen provider - it's not limited to a predefined list:

```yaml
config:
  provider: openai
  model: gpt-4-turbo-preview
  api_key: "{{ .env.OPENAI_API_KEY }}"
  params:
    temperature: 0.7
    max_tokens: 2000
```

### Provider-Specific Examples

<Tabs items={["OpenAI", "Anthropic", "Google", "Groq", "Ollama", "DeepSeek", "xAI"]} defaultValue="OpenAI">
  <Tab value="OpenAI">
    ```yaml
    config:
      provider: openai
      model: gpt-4-turbo-preview
      api_key: "{{ .env.OPENAI_API_KEY }}"
      params:
        temperature: 0.7
        max_tokens: 4000
        top_p: 0.9
        frequency_penalty: 0.1
        presence_penalty: 0.1
    ```
  </Tab>

  <Tab value="Anthropic">
    ```yaml
    config:
      provider: anthropic
      model: claude-4-opus
      api_key: "{{ .env.ANTHROPIC_API_KEY }}"
      params:
        temperature: 0.5
        max_tokens: 8000
        top_p: 0.9
    ```
  </Tab>

  <Tab value="Google">
    ```yaml
    config:
      provider: google
      model: gemini-pro
      api_key: "{{ .env.GOOGLE_API_KEY }}"
      params:
        temperature: 0.8
        max_tokens: 2048
        top_p: 0.95
    ```
  </Tab>

  <Tab value="Groq">
    ```yaml
    config:
      provider: groq
      model: llama-3.3-70b-versatile
      api_key: "{{ .env.GROQ_API_KEY }}"
      params:
        temperature: 0.7
        max_tokens: 1000
    ```
  </Tab>

  <Tab value="Ollama">
    ```yaml
    config:
      provider: ollama
      model: llama2:13b
      api_url: "http://localhost:11434"
      params:
        temperature: 0.6
        max_tokens: 2000
        repeat_penalty: 1.1
    ```
  </Tab>

  <Tab value="DeepSeek">
    ```yaml
    config:
      provider: deepseek
      model: deepseek-chat
      api_key: "{{ .env.DEEPSEEK_API_KEY }}"
      api_url: "https://api.deepseek.com/v1"  # Optional
      params:
        temperature: 0.7
        max_tokens: 4000
    ```
  </Tab>

  <Tab value="xAI">
    ```yaml
    config:
      provider: xai
      model: grok-beta
      api_key: "{{ .env.XAI_API_KEY }}"
      api_url: "https://api.x.ai/v1"  # Optional
      params:
        temperature: 0.7
        max_tokens: 4000
    ```
  </Tab>
</Tabs>

## Model Parameters

### Common Parameters

Most providers support these standard parameters. For the complete and most up-to-date parameter schema, see the [Provider Schema Documentation](/docs/schema/provider).

| Parameter | Type | Description | Range |
|-----------|------|-------------|-------|
| `temperature` | float | Controls randomness in generation | 0.0 - 2.0 |
| `max_tokens` | int | Maximum tokens to generate | Model-specific |
| `top_p` | float | Nucleus sampling threshold | 0.0 - 1.0 |
| `top_k` | int | Top-k sampling parameter | 1 - 100 |
| `frequency_penalty` | float | Reduces repetitive tokens | -2.0 - 2.0 |
| `presence_penalty` | float | Encourages new topics | -2.0 - 2.0 |

### Provider-Specific Parameters

<AccordionGroup>
  <Accordion title="OpenAI Parameters">
    <Table>
      <TableHeader>
        <TableRow>
          <TableHead>Parameter</TableHead>
          <TableHead>Type</TableHead>
          <TableHead>Description</TableHead>
        </TableRow>
      </TableHeader>
      <TableBody>
        <TableRow>
          <TableCell>`logit_bias`</TableCell>
          <TableCell>object</TableCell>
          <TableCell>Modify likelihood of specific tokens</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`user`</TableCell>
          <TableCell>string</TableCell>
          <TableCell>Unique identifier for end-user</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`seed`</TableCell>
          <TableCell>integer</TableCell>
          <TableCell>For reproducible outputs</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`response_format`</TableCell>
          <TableCell>object</TableCell>
          <TableCell>Force JSON output structure</TableCell>
        </TableRow>
      </TableBody>
    </Table>
  </Accordion>

  <Accordion title="Anthropic Parameters">
    <Table>
      <TableHeader>
        <TableRow>
          <TableHead>Parameter</TableHead>
          <TableHead>Type</TableHead>
          <TableHead>Description</TableHead>
        </TableRow>
      </TableHeader>
      <TableBody>
        <TableRow>
          <TableCell>`stop_sequences`</TableCell>
          <TableCell>array</TableCell>
          <TableCell>Custom stop sequences</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`metadata`</TableCell>
          <TableCell>object</TableCell>
          <TableCell>Request metadata</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`system`</TableCell>
          <TableCell>string</TableCell>
          <TableCell>System message (handled by instructions)</TableCell>
        </TableRow>
      </TableBody>
    </Table>
  </Accordion>

  <Accordion title="Ollama Parameters">
    <Table>
      <TableHeader>
        <TableRow>
          <TableHead>Parameter</TableHead>
          <TableHead>Type</TableHead>
          <TableHead>Description</TableHead>
        </TableRow>
      </TableHeader>
      <TableBody>
        <TableRow>
          <TableCell>`repeat_penalty`</TableCell>
          <TableCell>float</TableCell>
          <TableCell>Penalize repetitions</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`num_ctx`</TableCell>
          <TableCell>integer</TableCell>
          <TableCell>Context window size</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`num_predict`</TableCell>
          <TableCell>integer</TableCell>
          <TableCell>Prediction length</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`tfs_z`</TableCell>
          <TableCell>float</TableCell>
          <TableCell>Tail free sampling</TableCell>
        </TableRow>
      </TableBody>
    </Table>
  </Accordion>

  <Accordion title="Groq Parameters">
    <Table>
      <TableHeader>
        <TableRow>
          <TableHead>Parameter</TableHead>
          <TableHead>Type</TableHead>
          <TableHead>Description</TableHead>
        </TableRow>
      </TableHeader>
      <TableBody>
        <TableRow>
          <TableCell>`stream`</TableCell>
          <TableCell>boolean</TableCell>
          <TableCell>Enable streaming responses</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`stop`</TableCell>
          <TableCell>array</TableCell>
          <TableCell>Stop sequences for generation</TableCell>
        </TableRow>
      </TableBody>
    </Table>
  </Accordion>

  <Accordion title="Google Parameters">
    <Table>
      <TableHeader>
        <TableRow>
          <TableHead>Parameter</TableHead>
          <TableHead>Type</TableHead>
          <TableHead>Description</TableHead>
        </TableRow>
      </TableHeader>
      <TableBody>
        <TableRow>
          <TableCell>`top_k`</TableCell>
          <TableCell>integer</TableCell>
          <TableCell>Top-k sampling parameter</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`candidate_count`</TableCell>
          <TableCell>integer</TableCell>
          <TableCell>Number of candidates to generate</TableCell>
        </TableRow>
        <TableRow>
          <TableCell>`stop_sequences`</TableCell>
          <TableCell>array</TableCell>
          <TableCell>Sequences to stop generation</TableCell>
        </TableRow>
      </TableBody>
    </Table>
  </Accordion>
</AccordionGroup>

## Tool Iterations

When tools are available, agents can iterate between the LLM and tools multiple times to complete a task (e.g., read → analyze → write). To prevent infinite loops, Compozy applies iteration caps:

- Global cap: configured as `llm.max_tool_iterations` (see global configuration schema)
- Per-model override: `models[].max_tool_iterations` in provider config

If a per-model value is set, it overrides the global cap for that model.

<Callout type="info">
For provider-specific parameter validation and requirements, consult the [Provider Schema Documentation](/docs/schema/provider) which contains the authoritative schema definitions.
</Callout>

## Global Model Configuration

Define reusable model configurations in your project's `compozy.yaml`:

```yaml
models:
  - provider: openai
    model: gpt-4-turbo-preview
    api_key: "{{ .env.OPENAI_API_KEY }}"
    params:
      temperature: 0.7

  - provider: anthropic
    model: claude-4-opus
    api_key: "{{ .env.ANTHROPIC_API_KEY }}"

  - provider: ollama
    model: llama2:13b
    api_url: "http://localhost:11434"
```

Reference global models in agents:

```yaml
agents:
  - id: code-reviewer
    model: anthropic-claude-4-opus
    instructions: "Review code for quality..."
```

## Next Steps

<ReferenceCardList>
  <ReferenceCard
    title="Instructions & Actions"
    description="Learn how to craft effective agent instructions"
    href="/docs/core/agents/instructions-actions"
    icon="ListCheck"
  />
  <ReferenceCard
    title="Structured Outputs"
    description="Master schema-driven structured outputs and validation"
    href="/docs/core/agents/structured-outputs"
    icon="Braces"
  />
  <ReferenceCard
    title="Provider Configuration"
    description="Detailed provider setup and configuration options"
    href="/docs/core/configuration/providers"
    icon="Settings"
  />
  <ReferenceCard
    title="Provider Schema"
    description="Complete schema definitions and validation rules"
    href="/docs/schema/provider"
    icon="FileJson"
  />
</ReferenceCardList>
